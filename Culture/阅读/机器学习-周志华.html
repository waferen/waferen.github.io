<!DOCTYPE html> <html><head>
		<title>机器学习-周志华</title>
		<base href="..\../">
		<meta id="root-path" root-path="..\../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="note - 机器学习-周志华">
		<meta property="og:title" content="机器学习-周志华">
		<meta property="og:description" content="note - 机器学习-周志华">
		<meta property="og:type" content="website">
		<meta property="og:url" content="culture/阅读/机器学习-周志华.html">
		<meta property="og:image" content="undefined">
		<meta property="og:site_name" content="note">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="stylesheet" href="lib/styles/theme.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager theme-dark show-inline-title show-ribbon"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles">mjx-container[jax="CHTML"] { line-height: 0; }
mjx-container [space="1"] { margin-left: 0.111em; }
mjx-container [space="2"] { margin-left: 0.167em; }
mjx-container [space="3"] { margin-left: 0.222em; }
mjx-container [space="4"] { margin-left: 0.278em; }
mjx-container [space="5"] { margin-left: 0.333em; }
mjx-container [rspace="1"] { margin-right: 0.111em; }
mjx-container [rspace="2"] { margin-right: 0.167em; }
mjx-container [rspace="3"] { margin-right: 0.222em; }
mjx-container [rspace="4"] { margin-right: 0.278em; }
mjx-container [rspace="5"] { margin-right: 0.333em; }
mjx-container [size="s"] { font-size: 70.7%; }
mjx-container [size="ss"] { font-size: 50%; }
mjx-container [size="Tn"] { font-size: 60%; }
mjx-container [size="sm"] { font-size: 85%; }
mjx-container [size="lg"] { font-size: 120%; }
mjx-container [size="Lg"] { font-size: 144%; }
mjx-container [size="LG"] { font-size: 173%; }
mjx-container [size="hg"] { font-size: 207%; }
mjx-container [size="HG"] { font-size: 249%; }
mjx-container [width="full"] { width: 100%; }
mjx-box { display: inline-block; }
mjx-block { display: block; }
mjx-itable { display: inline-table; }
mjx-row { display: table-row; }
mjx-row > * { display: table-cell; }
mjx-mtext { display: inline-block; }
mjx-mstyle { display: inline-block; }
mjx-merror { display: inline-block; color: red; background-color: yellow; }
mjx-mphantom { visibility: hidden; }
mjx-assistive-mml { top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); user-select: none; position: absolute !important; padding: 1px 0px 0px !important; border: 0px !important; display: block !important; width: auto !important; overflow: hidden !important; }
mjx-assistive-mml[display="block"] { width: 100% !important; }
mjx-math { display: inline-block; text-align: left; line-height: 0; text-indent: 0px; font-style: normal; font-weight: normal; font-size: 100%; letter-spacing: normal; border-collapse: collapse; overflow-wrap: normal; word-spacing: normal; white-space: nowrap; direction: ltr; padding: 1px 0px; }
mjx-container[jax="CHTML"][display="true"] { display: block; text-align: center; margin: 1em 0px; }
mjx-container[jax="CHTML"][display="true"][width="full"] { display: flex; }
mjx-container[jax="CHTML"][display="true"] mjx-math { padding: 0px; }
mjx-container[jax="CHTML"][justify="left"] { text-align: left; }
mjx-container[jax="CHTML"][justify="right"] { text-align: right; }
mjx-mi { display: inline-block; text-align: left; }
mjx-c { display: inline-block; }
mjx-utext { display: inline-block; padding: 0.75em 0px 0.2em; }
mjx-mn { display: inline-block; text-align: left; }
mjx-mo { display: inline-block; text-align: left; }
mjx-stretchy-h { display: inline-table; width: 100%; }
mjx-stretchy-h > * { display: table-cell; width: 0px; }
mjx-stretchy-h > * > mjx-c { display: inline-block; transform: scaleX(1); }
mjx-stretchy-h > * > mjx-c::before { display: inline-block; width: initial; }
mjx-stretchy-h > mjx-ext { overflow: clip visible; width: 100%; }
mjx-stretchy-h > mjx-ext > mjx-c::before { transform: scaleX(500); }
mjx-stretchy-h > mjx-ext > mjx-c { width: 0px; }
mjx-stretchy-h > mjx-beg > mjx-c { margin-right: -0.1em; }
mjx-stretchy-h > mjx-end > mjx-c { margin-left: -0.1em; }
mjx-stretchy-v { display: inline-block; }
mjx-stretchy-v > * { display: block; }
mjx-stretchy-v > mjx-beg { height: 0px; }
mjx-stretchy-v > mjx-end > mjx-c { display: block; }
mjx-stretchy-v > * > mjx-c { transform: scaleY(1); transform-origin: left center; overflow: hidden; }
mjx-stretchy-v > mjx-ext { display: block; height: 100%; box-sizing: border-box; border: 0px solid transparent; overflow: visible clip; }
mjx-stretchy-v > mjx-ext > mjx-c::before { width: initial; box-sizing: border-box; }
mjx-stretchy-v > mjx-ext > mjx-c { transform: scaleY(500) translateY(0.075em); overflow: visible; }
mjx-mark { display: inline-block; height: 0px; }
mjx-c::before { display: block; width: 0px; }
.MJX-TEX { font-family: MJXZERO, MJXTEX; }
.TEX-B { font-family: MJXZERO, MJXTEX-B; }
.TEX-I { font-family: MJXZERO, MJXTEX-I; }
.TEX-MI { font-family: MJXZERO, MJXTEX-MI; }
.TEX-BI { font-family: MJXZERO, MJXTEX-BI; }
.TEX-S1 { font-family: MJXZERO, MJXTEX-S1; }
.TEX-S2 { font-family: MJXZERO, MJXTEX-S2; }
.TEX-S3 { font-family: MJXZERO, MJXTEX-S3; }
.TEX-S4 { font-family: MJXZERO, MJXTEX-S4; }
.TEX-A { font-family: MJXZERO, MJXTEX-A; }
.TEX-C { font-family: MJXZERO, MJXTEX-C; }
.TEX-CB { font-family: MJXZERO, MJXTEX-CB; }
.TEX-FR { font-family: MJXZERO, MJXTEX-FR; }
.TEX-FRB { font-family: MJXZERO, MJXTEX-FRB; }
.TEX-SS { font-family: MJXZERO, MJXTEX-SS; }
.TEX-SSB { font-family: MJXZERO, MJXTEX-SSB; }
.TEX-SSI { font-family: MJXZERO, MJXTEX-SSI; }
.TEX-SC { font-family: MJXZERO, MJXTEX-SC; }
.TEX-T { font-family: MJXZERO, MJXTEX-T; }
.TEX-V { font-family: MJXZERO, MJXTEX-V; }
.TEX-VB { font-family: MJXZERO, MJXTEX-VB; }
mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c { font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A !important; }
@font-face { font-family: MJXZERO; src: url("lib/fonts/mathjax_zero.woff") format("woff"); }
@font-face { font-family: MJXTEX; src: url("lib/fonts/mathjax_main-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-B; src: url("lib/fonts/mathjax_main-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-I; src: url("lib/fonts/mathjax_math-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-MI; src: url("lib/fonts/mathjax_main-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-BI; src: url("lib/fonts/mathjax_math-bolditalic.woff") format("woff"); }
@font-face { font-family: MJXTEX-S1; src: url("lib/fonts/mathjax_size1-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S2; src: url("lib/fonts/mathjax_size2-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S3; src: url("lib/fonts/mathjax_size3-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S4; src: url("lib/fonts/mathjax_size4-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-A; src: url("lib/fonts/mathjax_ams-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-C; src: url("lib/fonts/mathjax_calligraphic-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-CB; src: url("lib/fonts/mathjax_calligraphic-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-FR; src: url("lib/fonts/mathjax_fraktur-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-FRB; src: url("lib/fonts/mathjax_fraktur-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SS; src: url("lib/fonts/mathjax_sansserif-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSB; src: url("lib/fonts/mathjax_sansserif-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSI; src: url("lib/fonts/mathjax_sansserif-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-SC; src: url("lib/fonts/mathjax_script-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-T; src: url("lib/fonts/mathjax_typewriter-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-V; src: url("lib/fonts/mathjax_vector-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-VB; src: url("lib/fonts/mathjax_vector-bold.woff") format("woff"); }
mjx-c.mjx-c1D464.TEX-I::before { padding: 0.443em 0.716em 0.011em 0px; content: "w"; }
mjx-c.mjx-c31::before { padding: 0.666em 0.5em 0px 0px; content: "1"; }
mjx-c.mjx-c1D465.TEX-I::before { padding: 0.442em 0.572em 0.011em 0px; content: "x"; }
mjx-c.mjx-c2B::before { padding: 0.583em 0.778em 0.082em 0px; content: "+"; }
mjx-c.mjx-c32::before { padding: 0.666em 0.5em 0px 0px; content: "2"; }
mjx-c.mjx-c2E::before { padding: 0.12em 0.278em 0px 0px; content: "."; }
mjx-c.mjx-c1D45B.TEX-I::before { padding: 0.442em 0.6em 0.011em 0px; content: "n"; }
mjx-c.mjx-c2217::before { padding: 0.465em 0.5em 0px 0px; content: "∗"; }
mjx-c.mjx-c2212::before { padding: 0.583em 0.778em 0.082em 0px; content: "−"; }
mjx-c.mjx-c1D44F.TEX-I::before { padding: 0.694em 0.429em 0.011em 0px; content: "b"; }
mjx-c.mjx-c3D::before { padding: 0.583em 0.778em 0.082em 0px; content: "="; }
mjx-c.mjx-c30::before { padding: 0.666em 0.5em 0.022em 0px; content: "0"; }
</style><pre class="frontmatter language-yaml" tabindex="0" style="display: none;"><code class="language-yaml is-loaded"><span class="token key atrule">tags</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> 文化/阅读
<span class="token key atrule">书名：</span><span class="token punctuation">:</span> 机器学习
<span class="token key atrule">作者：</span><span class="token punctuation">:</span> 周志华
<span class="token key atrule">时间：</span><span class="token punctuation">:</span> 2024/09/09</code><button class="copy-code-button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-copy"><rect x="8" y="8" width="14" height="14" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></pre><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="机器学习-周志华">机器学习-周志华</h1><div class="el-h2 heading-wrapper"><h2 data-heading="摘录：" dir="auto" class="heading" id="摘录："><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>摘录：</h2><div class="heading-children"><div class="el-p"><p dir="auto">《机器学习》</p></div><div class="el-p"><p dir="auto">周志华<br>
149个笔记</p></div><div class="el-p"><p dir="auto">点评</p></div><div class="el-p"><p dir="auto">◆ 2024/04/14 认为一般</p></div><div class="el-p"><p dir="auto">其实我觉得作为教材还是有点跳跃了，有些式子的推导很跳跃让人很疑惑。比如BP算法里的gj突然出现，其实就是少了一个令字，让人很不解。但总体来说它构建了一个完整的机器学习知识框架。</p></div><div class="el-p"><p dir="auto">1.1 引言</p></div><div class="el-p"><p dir="auto">◆ 机器学习正是这样一门学科，它致力于研究如何通过计算的手段，利用经验来改善系统自身的性能，在计算机系统中，“经验”通常以“数据”形式存在，因此，机器学习所研究的主要内容，是关于在计算机上从数据中产生“模型”(model)的算法，即“学习算法”(learning algorithm)。</p></div><div class="el-p"><p dir="auto">1.2 基本术语</p></div><div class="el-p"><p dir="auto">◆ 由于空间中的每个点对应一个坐标向量，因此我们也把一个示例称为一个“特征向量”(feature vector)。</p></div><div class="el-p"><p dir="auto">◆ 从数据中学得模型的过程称为“学习”(learning)或“训练”(training)，这个过程通过执行某个学习算法来完成</p></div><div class="el-p"><p dir="auto">◆ 将“label”译为“标记”而非“标签”</p></div><div class="el-p"><p dir="auto">◆ 我们需获得训练样本的“结果”信息，例如“（（色泽=青绿；根蒂=蜷缩;敲声=浊响），好瓜）”。这里关于示例结果的信息，例如“好瓜”，称为“标记”(label)；拥有了标记信息的示例，则称为“样例”(example)。</p></div><div class="el-p"><p dir="auto">◆ 用(xi,yi)表示第i个样例，其中yi∈[插图]是示例xi的标记，[插图]是所有标记的集合，亦称“标记空间”(label space)或“输出空间”。亦称“负类”。</p></div><div class="el-p"><p dir="auto">◆ 若我们欲预测的是离散值，例如“好瓜”“坏瓜”，此类学习任务称为“分类”(classification)；若欲预测的是连续值，例如西瓜成熟度0.95、0.37，此类学习任务称为“回归”(regression)。</p></div><div class="el-p"><p dir="auto">◆ 学得模型后，使用其进行预测的过程称为“测试”(testing)，被预测的样本称为“测试样本”(testing sample)。</p></div><div class="el-p"><p dir="auto">◆ 我们还可以对西瓜做“聚类”(clustering)，即将训练集中的西瓜分成若干组，每组称为一个“簇”(cluster)；</p></div><div class="el-p"><p dir="auto">◆ 需说明的是，在聚类学习中，“浅色瓜”“本地瓜”这样的概念我们事先是不知道的，而且学习过程中使用的训练样本通常不拥有标记信息。</p></div><div class="el-p"><p dir="auto">◆ 根据训练数据是否拥有标记信息，学习任务可大致划分为两大类：“监督学习”(supervised learning)和“无监督学习”(unsupervised learning)，分类和回归是前者的代表，而聚类则是后者的代表。</p></div><div class="el-p"><p dir="auto">◆ 学得模型适用于新样本的能力，称为“泛化”(generalization)能力。</p></div><div class="el-p"><p dir="auto">◆ 通常假设样本空间中全体样本服从一个未知“分布”(distribution)[插图]，我们获得的每个样本都是独立地从这个分布上采样获得的，即“独立同分布”</p></div><div class="el-p"><p dir="auto">1.3 假设空间</p></div><div class="el-p"><p dir="auto">◆ 归纳(induction)与演绎(deduction)是科学推理的两大基本手段。前者是从特殊到一般的“泛化”(generalization)过程，即从具体的事实归结出一般性规律；后者则是从一般到特殊的“特化”(specialization)过程，即从基础原理推演出具体状况。</p></div><div class="el-p"><p dir="auto">◆ 。概念学习技术目前研究、应用都比较少，因为要学得泛化性能好且语义明确的概念实在太困难了，现实常用的技术大多是产生“黑箱”模型。</p></div><div class="el-p"><p dir="auto">◆ (A∧B)∨(C∧D)的析合范式。</p></div><div class="el-p"><p dir="auto">◆ “记住”训练样本，就能力。如果仅仅把训练集中的瓜“记住”，是所谓的“机械学习”[Cohen and Feigenbaum,1983]，或</p></div><div class="el-p"><p dir="auto">◆ 可以有许多策略对这个假设空间进行搜索，例如自顶向下、从一般到特殊，或是自底向上、从特殊到一般，搜索过程中可以不断删除与正例不一致的假设、和（或）与反例一致的假设。</p></div><div class="el-p"><p dir="auto">◆ 可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合”，我们称之为“版本空间”(version space)。</p></div><div class="el-p"><p dir="auto">2.3 性能度量</p></div><div class="el-p"><p dir="auto">◆ [插图]查准率P与查全率R分别定义为[插图]查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。</p></div><div class="el-p"><p dir="auto">◆ 以信息检索应用为例，逐条向用户反馈其可能感兴趣的信息，即可计算出查全率、查准率。</p></div><div class="el-p"><p dir="auto">◆ 2024/03/19发表想法</p></div><div class="el-p"><p dir="auto">以预测概率排序，划分临界值不断降低，每下降一次求一次P,R。最后平滑连接得到PR曲线。</p></div><div class="el-p"><p dir="auto">原文：在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为“最可能”是正例的样本，排在最后的则是学习器认为“最不可能”是正例的样本。按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率。</p></div><div class="el-p"><p dir="auto">◆ 在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为“最可能”是正例的样本，排在最后的则是学习器认为“最不可能”是正例的样本。按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率。</p></div><div class="el-p"><p dir="auto">◆ 以查准率为纵轴、查全率为横轴作图，就得到了查准率-查全率曲线，简称“P-R曲线”</p></div><div class="el-p"><p dir="auto">◆ 若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者</p></div><div class="el-p"><p dir="auto">◆ “平衡点”（Break-Even Point，简称BEP）就是这样一个度量，它是“查准率=查全率”时的取值</p></div><div class="el-p"><p dir="auto">◆ 但BEP还是过于简化了些，更常用的是F1度量：[插图]F1是基于查准率与查全率的调和平均(harmonicmean)定义的：[插图]</p></div><div class="el-p"><p dir="auto">◆ F1度量的一般形式――Fβ，能让我们表达出对查准率/查全率的不同偏好，它定义为[插图]Fβ则是加权调和平均：[插图]与算术平均[插图]和几何平均[插图]相比，调和平均更重视较小值。</p></div><div class="el-p"><p dir="auto">◆ β＞0度量了查全率对查准率的相对重要性[Van Rijsbergen,1979]。β=1时退化为标准的F1；β＞1时查全率有更大影响；β＜1时查准率有更大影响。</p></div><div class="el-p"><p dir="auto">◆ “宏查准率”(macro-P)、“宏查全率”(macro-R)，以及相应的“宏F1”(macro-F1)：[插图]</p></div><div class="el-p"><p dir="auto">◆ 分别记为[插图]，再基于这些平均值计算出“微查准率”(micro-P)、“微查全率”(micro-R)和“微F1”(micro-F1)：[插图][插图]</p></div><div class="el-p"><p dir="auto">◆ 很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值(threshold)进行比较，若大于阈值则分为正类，否则为反类。</p></div><div class="el-p"><p dir="auto">◆ ROC全称是“受试者工作特征”(Receiver Operating Characteristic)曲线，它源于“二战”中用于敌机检测的雷达信号分析技术，二十世纪六七十年代开始被用于一些心理学、医学检测应用中，此后被引入机器学习领域[Spackman,1989]</p></div><div class="el-p"><p dir="auto">◆ ROC曲线的纵轴是“真正例率”（True Positive Rate，简称TPR），横轴是“假正例率”（False Positive Rate，简称FPR），基于表2.1中的符号，两者分别定义为[插图]</p></div><div class="el-p"><p dir="auto">◆ 2024/03/19发表想法</p></div><div class="el-p"><p dir="auto">增加正例正确率时向上移动，增加反例错误率时往右移。</p></div><div class="el-p"><p dir="auto">原文：绘图过程很简单：给定m+个正例和m-个反例，根据学习器预测结果对样例进行排序，然后把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为0，在坐标(0,0)处标记一个点。然后，将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例。设前一个标记点坐标为(x,y)，当前若为真正例，则对应标记点的坐标为[插图]；当前若为假正例，则对应标记点的坐标为[插图]，然后用线段连接相邻点即得。</p></div><div class="el-p"><p dir="auto">◆ 绘图过程很简单：给定m+个正例和m-个反例，根据学习器预测结果对样例进行排序，然后把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为0，在坐标(0,0)处标记一个点。然后，将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例。设前一个标记点坐标为(x,y)，当前若为真正例，则对应标记点的坐标为[插图]；当前若为假正例，则对应标记点的坐标为[插图]，然后用线段连接相邻点即得。</p></div><div class="el-p"><p dir="auto">◆ 2024/03/19发表想法</p></div><div class="el-p"><p dir="auto">上底加下底乘高除二</p></div><div class="el-p"><p dir="auto">原文：此时如果一定要进行比较，则较为合理的判据是比较ROC曲线下的面积，即AUC(Area UnderROC Curve)，如图2.4所示。<br>
从定义可知，AUC可通过对ROC曲线下各部分的面积求和而得。假定ROC曲线是由坐标为{(x1,y1)，(x2,y2)，...，(xm,ym)}的点按序连接而形成(x1=0,xm=1)，参见图2.4(b)，则AUC可估算为<br>
[插图]</p></div><div class="el-p"><p dir="auto">◆ 此时如果一定要进行比较，则较为合理的判据是比较ROC曲线下的面积，即AUC(Area UnderROC Curve)，如图2.4所示。从定义可知，AUC可通过对ROC曲线下各部分的面积求和而得。假定ROC曲线是由坐标为{(x1,y1)，(x2,y2)，...，(xm,ym)}的点按序连接而形成(x1=0,xm=1)，参见图2.4(b)，则AUC可估算为[插图]</p></div><div class="el-p"><p dir="auto">◆ 2024/03/19发表想法</p></div><div class="el-p"><p dir="auto">往右走的时候产生了一个正方形即1的损失，分母为m+m-。即ROC曲线上半部分面积。</p></div><div class="el-p"><p dir="auto">原文：给定m+个正例和m-个反例，令D+和D-分别表示正、反例集合，则排序“损失”(loss)定义为<br>
[插图]</p></div><div class="el-p"><p dir="auto">◆ 给定m+个正例和m-个反例，令D+和D-分别表示正、反例集合，则排序“损失”(loss)定义为[插图]</p></div><div class="el-p"><p dir="auto">◆ [插图]rank对应的是ROC曲线之上的面积：若一个正例在ROC曲线上对应标记点的坐标为(x,y)，则x恰是排序在其之前的反例所占的比例，即假正例率。因此有AUC=1-[插图]rank。　(2.22)</p></div><div class="el-p"><p dir="auto">2.4 比较检验</p></div><div class="el-p"><p dir="auto">◆ 在很多时候我们并非仅做一次留出法估计，而是通过多次重复留出法或是交叉验证法等进行多次训练/测试，这样会得到多个测试错误率，此时可使用“t检验”(t-test)。假定我们得到了k个测试错误率，[插图]1,[插图]2,...,[插图]k，则平均测试错误率μ和方差σ2为[插图]考虑到这k个测试错误率可看作泛化错误率[插图]0的独立采样，则变量[插图]服从自由度为k-1的t分布，如图2.7所示。</p></div><div class="el-p"><p dir="auto">◆ [插图]e01=e10通常很小，需考虑连续性校正，因此分子中有-1项。若我们做的假设是两学习器性能相同，则应有e01=e10，那么变量|e01-e10|应当服从正态分布。McNemar检验考虑变量[插图]中文称为“卡方分布”。</p></div><div class="el-p"><p dir="auto">2.5 偏差与方差</p></div><div class="el-p"><p dir="auto">◆ 泛化误差可分解为偏差、方差与噪声之和。</p></div><div class="el-p"><p dir="auto">◆ 回顾偏差、方差、噪声的含义：偏差(2.40)度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；方差(2.38)度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；噪声(2.39)则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p></div><div class="el-p"><p dir="auto">3.1 基本形式</p></div><div class="el-p"><p dir="auto">◆ 2024/03/26发表想法</p></div><div class="el-p"><p dir="auto">在多元线性回归中，3.1式子中w和x都是向量，x代表不同特性向量，w则是权重向量。</p></div><div class="el-p"><p dir="auto">原文：f(x)=w1x1+w2x2+...+wdxd+b,　(3.1)<br>
一般用向量形式写成<br>
f(x)=wTx+b,　(3.2)</p></div><div class="el-p"><p dir="auto">◆ f(x)=w1x1+w2x2+...+wdxd+b,　(3.1)一般用向量形式写成f(x)=wTx+b,　(3.2)</p></div><div class="el-p"><p dir="auto">◆ 许多功能更为强大的非线性模型(nonlinear model)可在线性模型的基础上通过引入层级结构或高维映射而得。</p></div><div class="el-p"><p dir="auto">3.2 线性回归</p></div><div class="el-p"><p dir="auto">◆ “线性回归”(linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记。</p></div><div class="el-p"><p dir="auto">◆ 均方误差亦称平方损失(square loss)。</p></div><div class="el-p"><p dir="auto">◆ 试图让均方误差最小化，即[插图]w<em>,b</em>表示w和b的解。最小二乘法用途很广，不仅限于线性回归。</p></div><div class="el-p"><p dir="auto">◆ 令式(3.5)和(3.6)为零可得到w和b最优解的闭式(closed-form)解[插图][插图]其中[插图]为x的均值。</p></div><div class="el-p"><p dir="auto">◆ f(xi)=wTxi+b，使得f(xi)[插图]yi。亦称“多变量线性回归”。这称为“多元线性回归”(multivariate linear regression)。</p></div><div class="el-p"><p dir="auto">◆ 我们把w和b吸收入向量形式[插图]=(w;b)，相应的，把数据集D表示为一个m×(d+1)大小的矩阵X，其中每行对应于一个示例，该行前d个元素对应于示例的d个属性值，最后一个元素恒置为1，即[插图]再把标记也写成向量形式y=(y1;y2;...;ym)，则类似于式(3.4)，有[插图]令上式为零可得[插图]最优解的闭式解，但由于涉及矩阵逆的计算，比单变量情形要复杂一些。下面我们做一个简单的讨论。当XTX为满秩矩阵(full-rank matrix)或正定矩阵(positive definite matrix)时，令式(3.10)为零可得[插图]*=(XTX)-1XTy,(3-11)其中(XTX)-1是矩阵(XTX)的逆矩阵。令[插图]i=(xi;1)，则最终学得的多元线性回归模型为[插图]</p></div><div class="el-p"><p dir="auto">◆ 现实任务中XTX往往不是满秩矩阵。</p></div><div class="el-p"><p dir="auto">◆ 2024/03/26发表想法</p></div><div class="el-p"><p dir="auto">岭回归在最小二乘法后面加了个L2范数的平方，让XTX始终可逆，而Lasso回归则是在最小二乘法后面加了个L1范数。ps:L1范数即绝对值之和。L2范数即平方和开根号。是否有同时加两种范数的回归方法呢，有，Elastic回归。</p></div><div class="el-p"><p dir="auto">原文：选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化(regularization)项。</p></div><div class="el-p"><p dir="auto">◆ 2024/03/26发表想法</p></div><div class="el-p"><p dir="auto">岭回归</p></div><div class="el-p"><p dir="auto">原文：选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化(regularization)项。</p></div><div class="el-p"><p dir="auto">◆ 选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化(regularization)项。</p></div><div class="el-p"><p dir="auto">◆ 2024/03/26发表想法</p></div><div class="el-p"><p dir="auto">广义线性回归</p></div><div class="el-p"><p dir="auto">原文：可否令模型预测值逼近y的衍生物呢？譬如说，假设我们认为示例所对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即<br>
lny=wTx+b.　(3.14)<br>
这就是“对数线性回归”(log-linear regression)，它实际上是在试图让ewTx+b逼近y。</p></div><div class="el-p"><p dir="auto">◆ 可否令模型预测值逼近y的衍生物呢？譬如说，假设我们认为示例所对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即lny=wTx+b.　(3.14)这就是“对数线性回归”(log-linear regression)，它实际上是在试图让ewTx+b逼近y。</p></div><div class="el-p"><p dir="auto">◆ 但实质上已是在求取输入空间到输出空间的非线性函数映射，</p></div><div class="el-p"><p dir="auto">◆ 2024/04/01发表想法</p></div><div class="el-p"><p dir="auto">在线性模型的基础上套一个单调可微的函数</p></div><div class="el-p"><p dir="auto">原文：考虑单调可微函数g(·)，令<br>
y=g-1(wTx+b),　(3.15)</p></div><div class="el-p"><p dir="auto">◆ 考虑单调可微函数g(·)，令y=g-1(wTx+b),　(3.15)</p></div><div class="el-p"><p dir="auto">◆ 其中函数g(·)称为“联系函数”(link function)。显然，对数线性回归是广义线性模型在g(·)=ln(·)时的特例。</p></div><div class="el-p"><p dir="auto">3.3 对数几率回归</p></div><div class="el-p"><p dir="auto">◆ 只需找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。亦称Heaviside函数。</p></div><div class="el-p"><p dir="auto">◆ 最理想的是“单位阶跃函数”(unit-step function)[插图]即若预测值z大于零就判为正例，小于零则判为反例，预测值为临界值零则可任意判别，如图3.2所示。</p></div><div class="el-p"><p dir="auto">◆ 2024/03/26发表想法</p></div><div class="el-p"><p dir="auto">这里的z就是y(x)，把原本的回归问题得到的结果转化为分类的依据。</p></div><div class="el-p"><p dir="auto">原文：对数几率函数(logistic function)正是这样一个常用的替代函数：</p></div><div class="el-p"><p dir="auto">◆ 对数几率函数(logistic function)正是这样一个常用的替代函数：[插图]</p></div><div class="el-p"><p dir="auto">◆ Sigmoid函数即形似S的函数。对率函数是Sigmoid函数最重要的代表，在（第5章 神经网络）将看到它在神经网络中的重要作用。</p></div><div class="el-p"><p dir="auto">◆ 2024/03/26发表想法</p></div><div class="el-p"><p dir="auto">好瓜的概率比上坏瓜的概率取对数</p></div><div class="el-p"><p dir="auto">原文：[插图]</p></div><div class="el-p"><p dir="auto">◆ [插图]</p></div><div class="el-p"><p dir="auto">◆ 若将y视为样本x作为正例的可能性，则1-y是其反例可能性，两者的比值[插图]称为“几率”(odds)，反映了x作为正例的相对可能性。对几率取对数则得到“对数几率”（log odds，亦称logit）</p></div><div class="el-p"><p dir="auto">◆ 式(3.18)实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为“对数几率回归”（logisticregression，亦称logit regression）。特别需注意到，虽然它的名字是“回归”，但实际却是一种分类学习方法。</p></div><div class="el-p"><p dir="auto">◆ 我们可通过“极大似然法”(maximum likelihood method)来估计w和b。给定数据集[插图]，对率回归模型最大化“对数似然”(loglikelihood)</p></div><div class="el-p"><p dir="auto">◆ 2024/03/26发表想法</p></div><div class="el-p"><p dir="auto">为什么要求积再取对数变成加和形式，我怎么不直接对似然求和？</p></div><div class="el-p"><p dir="auto">原文：[插图]<br>
即令每个样本属于其真实标记的概率越大越好。</p></div><div class="el-p"><p dir="auto">◆ [插图]即令每个样本属于其真实标记的概率越大越好。</p></div><div class="el-p"><p dir="auto">3.4 线性判别分析</p></div><div class="el-p"><p dir="auto">◆ 线性判别分析（Linear Discriminant Analysis，简称LDA）</p></div><div class="el-p"><p dir="auto">◆ 2024/03/26发表想法</p></div><div class="el-p"><p dir="auto">不同类中心距离越大越好，同类协方差越小越好</p></div><div class="el-p"><p dir="auto">原文：LDA的思想非常朴素：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。</p></div><div class="el-p"><p dir="auto">◆ LDA的思想非常朴素：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。</p></div><div class="el-p"><p dir="auto">3.5 多分类学习</p></div><div class="el-p"><p dir="auto">◆ 最经典的拆分策略有三种：“一对一”（One vs.One，简称OvO）、“一对其余”（One vs.Rest，简称OvR）和“多对多”（Many vs.Many，简称MvM）。</p></div><div class="el-p"><p dir="auto">◆ 为什么称为“纠错输出码”呢？这是因为在测试阶段，ECOC编码对分类器的错误有一定的容忍和修正能力。例如图3.5(a)中对测试示例的正确预测编码是(-1,+1,+1,-1,+1)，假设在预测时某个分类器出错了，例如f2出错从而导致了错误编码(-1,-1,+1,-1,+1)，但基于这个编码仍能产生正确的最终分类结果C3。一般来说，对同一个学习任务，ECOC编码越长，纠错能力越强。然而，编码越长，意味着所需训练的分类器越多，计算、存储开销都会增大；另一方面，对有限类别数，可能的组合数目是有限的，码长超过一定范围后就失去了意义。</p></div><div class="el-p"><p dir="auto">◆ 对同等长度的编码，理论上来说，任意两个类别之间的编码距离越远，则纠错能力越强。因此，在码长较小时可根据这个原则计算出理论最优编码。然而，码长稍大一些就难以有效地确定最优编码，事实上这是NP难问题。</p></div><div class="el-p"><p dir="auto">3.6 类别不平衡问题</p></div><div class="el-p"><p dir="auto">◆ 类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况。</p></div><div class="el-p"><p dir="auto">◆ [插图]无偏采样意味着真实样本总体的类别比例在训练集中得以保持。然而，当训练集中正、反例的数目不同时，令m+表示正例数目，m-表示反例数目，则观测几率是[插图]，由于我们通常假设训练集是真实样本总体的无偏采样，因此观测几率就代表了真实几率。于是，只要分类器的预测几率高于观测几率就应判定为正例，即[插图]但是，我们的分类器是基于式(3.46)进行决策，因此，需对其预测值进行调整，使其在基于式(3.46)决策时，实际是在执行式(3.47)。要做到这一点很容易，只需令[插图]亦称“再平衡”(rebalance)。这就是类别不平衡学习的一个基本策略――“再缩放”(rescaling)。</p></div><div class="el-p"><p dir="auto">◆ 主要因为“训练集是真实样本总体的无偏采样”这个假设往往并不成立，也就是说，我们未必能有效地基于训练集观测几率来推断出真实几率。</p></div><div class="el-p"><p dir="auto">◆ 第一类是直接对训练集里的反类样例进行“欠采样”(undersampling)，即去除一些反例使得正、反例数目接近，然后再进行学习；第二类是对训练集里的正类样例进行“过采样”(oversampling)，即增加一些正例使得正、反例数目接近，然后再进行学习；第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将式(3.48)嵌入到其决策过程中，称为“阈值移动”(threshold-moving)。</p></div><div class="el-p"><p dir="auto">◆ 需注意的是，过采样法不能简单地对初始正例样本进行重复采样，否则会招致严重的过拟合；过采样法的代表性算法SMOTE[Chawlaet al.,2002]是通过对训练集里的正例进行插值来产生额外的正例。另一方面，欠采样法若随机丢弃反例，可能丢失一些重要信息；欠采样法的代表性算法EasyEnsemble[Liu et al.,2009]则是利用集成学习机制，将反例划分为若干个集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息。</p></div><div class="el-p"><p dir="auto">◆ “再缩放”也是“代价敏感学习”(cost-sensitive learning)的基础。在代价敏感学习中将式(3.48)中的m-/m+用cost+/cost-代替即可，其中cost+是将正例误分为反例的代价，cost-是将反例误分为正例的代价。</p></div><div class="el-p"><p dir="auto">3.7 阅读材料</p></div><div class="el-p"><p dir="auto">◆ 多分类学习中虽然有多个类别，但每个样本仅属于一个类别。如果希望为一个样本同时预测出多个类别标记，例如一幅图像可同时标注为“蓝天”、“白云”、“羊群”、“自然场景”，这样的任务就不再是多分类学习，而是“多标记学习”(multi-label learning)</p></div><div class="el-p"><p dir="auto">4.2 划分选择</p></div><div class="el-p"><p dir="auto">◆ “编号”将产生17个分支，每个分支结点仅包含一个样本，这些分支结点的纯度已达最大。然而，这样的决策树显然不具有泛化能力，无法对新样本进行有效预测。</p></div><div class="el-p"><p dir="auto">4.3 剪枝处理</p></div><div class="el-p"><p dir="auto">◆ 剪枝(pruning)是决策树学习算法对付“过拟合”的主要手段</p></div><div class="el-p"><p dir="auto">◆ 预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点</p></div><div class="el-p"><p dir="auto">◆ 如何判断决策树泛化性能是否提升呢？这可使用2.2节介绍的性能评估方法。本节假定采用留出法，即预留一部分数据用作“验证集”以进行性能评估。</p></div><div class="el-p"><p dir="auto">◆ 在划分之前，所有样例集中在根结点。若不进行划分，则根据算法4.2第6行，该结点将被标记为叶结点，其类别标记为训练样例数最多的类别，假设我们将这个叶结点标记为“好瓜”</p></div><div class="el-p"><p dir="auto">◆ 预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险。</p></div><div class="el-p"><p dir="auto">◆ 对结点②，若将其领衔的子树替换为叶结点，则替换后的叶结点包含编号为{1,2,3,14}的训练样例，叶结点标记为“好瓜”。此时决策树的验证集精度提高至71.4%。于是，后剪枝策略决定剪枝。</p></div><div class="el-p"><p dir="auto">◆ 一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。</p></div><div class="el-p"><p dir="auto">4.4 连续与缺失值</p></div><div class="el-p"><p dir="auto">◆ 我们需解决两个问题：(1)如何在属性值缺失的情况下进行划分属性选择？(2)给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？</p></div><div class="el-p"><p dir="auto">5.1 神经元模型</p></div><div class="el-p"><p dir="auto">◆ 在这个模型中，神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接(connection)进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过“激活函数”(activation function)处理以产生神经元的输出。[插图]</p></div><div class="el-p"><p dir="auto">5.2 感知机与多层网络</p></div><div class="el-p"><p dir="auto">◆ 感知机(Perceptron)由两层神经元组成，如图5.3所示，输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，亦称“阈值逻辑单元”(threshold logic unit)。</p></div><div class="el-p"><p dir="auto">◆ 更一般地，给定训练数据集，权重wi(i=1,2,...,n)以及阈值[插图]可通过学习得到。阈值[插图]可看作一个固定输入为-1.0的“哑结点”(dummy node)所对应的连接权重wn+1，这样，权重和阈值的学习就可统一为权重的学习</p></div><div class="el-p"><p dir="auto">◆ 感知机学习规则非常简单，对训练样例(x,y)，若当前感知机的输出为[插图]，则感知机权重将这样调整：[插图]xi是x对应于第i个输η通常设置为一个小正数，例如0.1。其中η∈(0,1)称为学习率(learning rate).从式(5.1)可看出，若感知机对训练样例(x,y)预测正确，即[插图]=y，则感知机不发生变化，否则将根据错误的程度进行权重调整</p></div><div class="el-p"><p dir="auto">◆ 感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元(functional neuron)，其学习能力非常有限。事实上，上述与、或、非问题都是线性可分(linearly separable)的问题。</p></div><div class="el-p"><p dir="auto">◆ “前馈”并不意味着网络中信号不能向后传，而是指网络拓扑结构上不存在环或回路；</p></div><div class="el-p"><p dir="auto">5.3 误差逆传播算法</p></div><div class="el-p"><p dir="auto">◆ 误差逆传播（error BackPropagation，简称BP）算法就是其中最杰出的代表，它是迄今最成功的神经网络学习算法。</p></div><div class="el-p"><p dir="auto">◆ BP算法不仅可用于多层前馈神经网络，还可用于其他类型的神经网络，例如训练递归神经网络</p></div><div class="el-p"><p dir="auto">6.1 间隔与支持向量</p></div><div class="el-p"><p dir="auto">◆ 换言之，这个划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强。</p></div><div class="el-p"><p dir="auto">◆ 2024/04/14发表想法</p></div><div class="el-p"><p dir="auto">在支持向量机（SVM）中，超平面被用作决策边界以区分不同的类别。在n维空间中，超平面可以定义为n-1维的子空间，即一个维度比整个空间少的子集。<br>
超平面通常用线性方程来表示，在二维空间中，超平面就是一条线，可以由方程 y = ax + b 表示；在三维空间中，超平面是一个平面，可由方程 z = ax + by + c 表示。<br>
对于更高维度的空间，超平面可由以下线性方程来表示：<br>
<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-math></mjx-container></span><br>
其中，向量 w = (w1, w2, …, wn) 是超平面的法向量，决定了超平面的方向；x = (x1, x2, …, xn) 是空间内的任意一点；b 是偏置项，决定了超平面沿法向量方向的位置。变量 w 和 b 是SVM训练过程中要学习的参数。根据这个线性方程，我们能够判断一个点位于超平面的哪一侧，因此可以用来进行分类。</p></div><div class="el-p"><p dir="auto">原文：在样本空间中，划分超平面可通过如下线性方程来描述：[插图]</p></div><div class="el-p"><p dir="auto">◆ 在样本空间中，划分超平面可通过如下线性方程来描述：[插图]</p></div><div class="el-p"><p dir="auto">◆ 其中w=(w1;w2;...;wd)为法向量，决定了超平面的方向；b为位移项，决定了超平面与原点之间的距离。</p></div><div class="el-p"><p dir="auto">◆ 任意点x到超平面(w,b)的距离可写为[插图]</p></div><div class="el-p"><p dir="auto">6.2 对偶问题</p></div><div class="el-p"><p dir="auto">◆ 其中w和b是模型参数。注意到式(6.6)本身是一个凸二次规划(convex quadratic programming)问题，能直接用现成的优化计算包求解，但我们可以有更高效的办法。参见附录（B.2 二次规划）。对式(6.6)使用拉格朗日乘子法可得到其“对偶问题”(dual problem)。</p></div><div class="el-p"><p dir="auto">◆ KKT(Karush-Kuhn-Tucker)条件，即要求[插图]如[Vapnik,1999]所述，支持向量机这个名字强调了此类学习器的关键是如何从支持向量构建出解；同时也暗示着其复杂度主要与支持向量的数目有关。</p></div><div class="el-p"><p dir="auto">6.5 支持向量回归</p></div><div class="el-p"><p dir="auto">◆ 对样本(x,y)，传统回归模型通常直接基于模型输出f(x)与真实输出y之间的差别来计算损失，当且仅当f(x)与y完全相同时，损失才为零。与此不同，支持向量回归（Support Vector Regression，简称SVR）假设我们能容忍f(x)与y之间最多有[插图]的偏差，即仅当f(x)与y之间的差别绝对值大于[插图]时才计算损失。</p></div><div class="el-p"><p dir="auto">6.6 核方法</p></div><div class="el-p"><p dir="auto">◆ 给定训练样本{(x1,y1)，(x2,y2)，...，(xm,ym)}，若不考虑偏移项b，则无论SVM还是SVR，学得的模型总能表示成核函数k(x,xi)的线性组合。</p></div><div class="el-p"><p dir="auto">◆ 表示定理对损失函数没有限制，对正则化项Ω仅要求单调递增，甚至不要求Ω是凸函数，意味着对于一般的损失函数和正则化项，优化问题(6.57)的最优解ℎ*(x)都可表示为核函数k(x,xi)的线性组合；这显示出核函数的巨大威力。</p></div><div class="el-p"><p dir="auto">7.1 贝叶斯决策论</p></div><div class="el-p"><p dir="auto">◆ 对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。</p></div><div class="el-p"><p dir="auto">◆ 假设有N种可能的类别标记，即[插图]={c1,c2，…，cN}，λij是将一个真实标记为cj的样本误分类为ci所产生的损失。基于后验概率P(ci|x)可获得将样本x分类为ci所产生的期望损失(expected loss)，即在样本x上的“条件风险”(conditional risk)[插图]</p></div><div class="el-p"><p dir="auto">◆ 我们的任务是寻找一个判定准则h:x[插图]y以最小化总体风险</p></div><div class="el-p"><p dir="auto">◆ 首先要获得后验概率P(c|x)。然而，在现实任务中这通常难以直接获得。</p></div><div class="el-p"><p dir="auto">7.2 极大似然估计</p></div><div class="el-p"><p dir="auto">◆ 极大似然估计是试图在θc所有可能的取值中，找到一个能使数据出现的“可能性”最大的值。</p></div><div class="el-p"><p dir="auto">7.4 半朴素贝叶斯分类器</p></div><div class="el-p"><p dir="auto">◆ 半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息，从而既不需进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。</p></div><div class="el-p"><p dir="auto">8.1 个体与集成</p></div><div class="el-p"><p dir="auto">◆ 集成学习(ensemble learning)通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统(multi-classifier system)、基于委员会的学习(committee-based learning)等。</p></div><div class="el-p"><p dir="auto">◆ 例如“决策树集成”中全是决策树，“神经网络集成”中全是神经网络，这样的集成是“同质”的(homogeneous)。同质集成中的个体学习器亦称“基学习器”(base learner)，相应的学习算法称为“基学习算法”(base learning algorithm)。集成也可包含不同类型的个体学习器，例如同时包含决策树和神经网络，这样的集成是“异质”的(heterogenous)。</p></div><div class="el-p"><p dir="auto">◆ 集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。这对“弱学习器”(weak learner)尤为明显，因此集成学习的很多理论研究都是针对弱学习器进行的，而基学习器有时也被直接称为弱学习器。</p></div><div class="el-p"><p dir="auto">◆ 集成学习的结果通过投票法(voting)产生，即“少数服从多数”。在图8.2(a)中，每个分类器都只有66.6%的精度，但集成学习却达到了100%</p></div><div class="el-p"><p dir="auto">◆ ；在图8.2(b)中，三个分类器没有差别，集成之后性能没有提高；在图8.2(c)中，每个分类器的精度都只有33.3%，集成学习的结果变得更糟</p></div><div class="el-p"><p dir="auto">◆ 要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”(diversity)，即学习器间具有差异。</p></div><div class="el-p"><p dir="auto">◆ 假设集成通过简单投票法结合T个基分类器，若有超过半数的基分类器正确，则集成分类就正确：[插图]参见习题8.1。假设基分类器的错误率相互独立，则由Hoeffding不等式可知，集成的错误率为[插图]上式显示出，随着集成中个体分类器数目T的增大，集成的错误率将指数级下降，最终趋向于零。</p></div><div class="el-p"><p dir="auto">8.3 Bagging与随机森林</p></div><div class="el-p"><p dir="auto">◆ 2024/05/07发表想法</p></div><div class="el-p"><p dir="auto">放回采样是为了防止出现偏执</p></div><div class="el-p"><p dir="auto">原文：Bagging[Breiman,1996a]是并行式集成学习方法最著名的代表。从名字即可看出，它直接基于我们在2.2.3节介绍过的自助采样法(bootstrap sampling)。给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。</p></div><div class="el-p"><p dir="auto">◆ Bagging[Breiman,1996a]是并行式集成学习方法最著名的代表。从名字即可看出，它直接基于我们在2.2.3节介绍过的自助采样法(bootstrap sampling)。给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。</p></div><div class="el-p"><p dir="auto">◆ 我们可采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。这就是Bagging的基本流程。</p></div><div class="el-p"><p dir="auto">8.6 阅读材料</p></div><div class="el-p"><p dir="auto">◆ Boosting主要关注降低偏差，而Bagging主要关注降低方差</p></div><div class="el-p"><p dir="auto">9.3 距离计算</p></div><div class="el-p"><p dir="auto">◆ p=2时，闵可夫斯基距离即欧氏距离(Euclidean distance)</p></div><div class="el-p"><p dir="auto">10.6 度量学习</p></div><div class="el-p"><p dir="auto">◆ 在机器学习中，对高维数据进行降维的主要目的是希望找到一个合适的低维空间，在此空间中进行学习能比原始空间性能更好。事实上，每个空间对应了在样本属性上定义的一个距离度量，而寻找合适的空间，实质上就是在寻找一个合适的距离度量。那么，为何不直接尝试“学习”出一个合适的距离度量呢？这就是度量学习(metric learning)的基本动机。</p></div><div class="el-p"><p dir="auto">14.1 隐马尔可夫模型</p></div><div class="el-p"><p dir="auto">◆ 但推断远超出预测范畴，例如在吃到一个不见根蒂的好瓜时，“由果溯因”逆推其根蒂的状态也是推断。</p></div><div class="el-p"><p dir="auto">◆ 机器学习最重要的任务，是根据一些已观察到的证据（例如训练样本）来对感兴趣的未知变量（例如类别标记）进行估计和推测</p></div><div class="el-p"><p dir="auto">◆ 概率模型(probabilistic model)提供了一种描述框架，将学习任务归结于计算变量的概率分布。</p></div><div class="el-p"><p dir="auto">◆ 若变量间存在显式的因果关系，则常使用贝叶斯网；若变量间存在相关性，但难以获得显式的因果关系，则常使用马尔可夫网。</p></div><div class="el-p"><p dir="auto">◆ 第一类是使用有向无环图表示变量间的依赖关系，称为有向图模型或贝叶斯网(Bayesian network)；第二类是使用无向图表示变量间的相关关系，称为无向图模型或马尔可夫网(Markov network)。</p></div><div class="el-p"><p dir="auto">◆ 箭头表示了变量间的依赖关系。在任一时刻，观测变量的取值仅依赖于状态变量，即xt由yt确定，与其他状态变量及观测变量的取值无关。</p></div><div class="el-p"><p dir="auto">◆ 这就是所谓的“马尔可夫链”(Markov chain)，即：系统下一时刻的状态仅由当前状态决定，不依赖于以往的任何状态。</p></div><div class="el-p"><p dir="auto">◆ 基于这种依赖关系，所有变量的联合概率分布为[插图]</p></div><div class="el-p"><p dir="auto">◆ 隐马尔可夫模型还需以下三组参数：• 状态转移概率：模型在各个状态间转换的概率，通常记为矩阵A=[aij]N×N，其中[插图]表示在任意时刻t,若状态为si，则在下一时刻状态为sj的概率。• 输出观测概率：模型根据当前状态获得各个观测值的概率，通常记为矩阵B=[bij]N×M，其中[插图]表示在任意时刻t,若状态为si，则观测值oj被获取的概率。• 初始状态概率：模型在初始时刻各状态出现的概率，通常记为π=(π1，π2,...，πN)，其中πI=P(y1=si),1≤i≤N表示模型的初始状态为si的概率。</p></div><div class="el-p"><p dir="auto">B 优化</p></div><div class="el-p"><p dir="auto">◆ 从几何角度看，该问题的目标是在由方程g([插图])=0确定的d-1维曲面上寻找能使目标函数<a class="internal-link" data-href="[插图]" href="[插图]" target="_self" rel="noopener nofollow">插图</a>最小化的点。此时不难得到如下结论。</p></div><div class="el-p"><p dir="auto">◆ 若梯度∇<a class="internal-link" data-href="[插图]*" href="[插图]*" target="_self" rel="noopener nofollow">插图</a>与约束曲面不正交，则仍可在约束曲面上移动该点使函数值进一步下降。</p></div><div class="el-p"><p dir="auto">◆ 对等式约束，λ可能为正也可能为负。对于约束曲面上的任意点[插图]，该点的梯度∇g([插图])正交于约束曲面。在最优点[插图]<em>，目标函数在该点的梯度∇[插图]([插图]</em>)正交于约束曲面。</p></div><div class="el-p"><p dir="auto">后记</p></div><div class="el-p"><p dir="auto">◆ 图灵奖得主E.W.Dijkstra曾说“计算机科学并不仅是关于计算机，就像天文学并不仅是关于望远镜”。</p></div><div class="el-p"><p dir="auto">-- 来自微信读书</p></div></div></div><div class="el-h2 heading-wrapper"><h2 data-heading="阅读感受：" dir="auto" class="heading" id="阅读感受："><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>阅读感受：</h2><div class="heading-children"><div class="mod-footer mod-ui"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="culture\阅读\机器学习-周志华.html#机器学习-周志华"><div class="tree-item-contents heading-link" heading-name="机器学习-周志华"><span class="tree-item-title">机器学习-周志华</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="culture\阅读\机器学习-周志华.html#摘录："><div class="tree-item-contents heading-link" heading-name="摘录："><span class="tree-item-title">摘录：</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="culture\阅读\机器学习-周志华.html#阅读感受："><div class="tree-item-contents heading-link" heading-name="阅读感受："><span class="tree-item-title">阅读感受：</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>