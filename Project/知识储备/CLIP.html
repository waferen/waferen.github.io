<!DOCTYPE html> <html><head>
		<title>CLIP</title>
		<base href="..\../">
		<meta id="root-path" root-path="..\../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="note - CLIP">
		<meta property="og:title" content="CLIP">
		<meta property="og:description" content="note - CLIP">
		<meta property="og:type" content="website">
		<meta property="og:url" content="project/知识储备/clip.html">
		<meta property="og:image" content="https://pic2.zhimg.com/80/v2-b86361b47d4db80258439b8ad33bdf8d_1440w.webp">
		<meta property="og:site_name" content="note">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="stylesheet" href="lib/styles/theme.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager theme-dark show-inline-title show-ribbon"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles">mjx-texatom{display:inline-block;text-align:left}mjx-msub{display:inline-block;text-align:left}mjx-msup{display:inline-block;text-align:left}mjx-msubsup{display:inline-block;text-align:left}mjx-script{display:inline-block;padding-right:.05em;padding-left:.033em}mjx-script>mjx-spacer{display:block}mjx-c.mjx-c2F::before{padding:.75em .5em .25em 0;content:"/"}mjx-c.mjx-c2211.TEX-S1::before{padding:.75em 1.056em .25em 0;content:"∑"}mjx-c.mjx-c28::before{padding:.75em .389em .25em 0;content:"("}mjx-c.mjx-c1D466.TEX-I::before{padding:.442em .49em .205em 0;content:"y"}mjx-c.mjx-c1D456.TEX-I::before{padding:.661em .345em .011em 0;content:"i"}mjx-c.mjx-c1D703.TEX-I::before{padding:.705em .469em .01em 0;content:"θ"}mjx-c.mjx-c1D447.TEX-I::before{padding:.677em .704em 0 0;content:"T"}mjx-c.mjx-c29::before{padding:.75em .389em .25em 0;content:")"}mjx-c.mjx-c1D6FC.TEX-I::before{padding:.442em .64em .011em 0;content:"α"}mjx-c.mjx-c1D457.TEX-I::before{padding:.661em .412em .204em 0;content:"j"}mjx-c.mjx-c1D44A.TEX-I::before{padding:.683em 1.048em .022em 0;content:"W"}mjx-c.mjx-c1D44C.TEX-I::before{padding:.683em .763em 0 0;content:"Y"}mjx-c.mjx-c1D44B.TEX-I::before{padding:.683em .852em 0 0;content:"X"}mjx-c.mjx-c1D437.TEX-I::before{padding:.683em .828em 0 0;content:"D"}mjx-c.mjx-c1D45A.TEX-I::before{padding:.442em .878em .011em 0;content:"m"}mjx-c.mjx-c1D43F.TEX-I::before{padding:.683em .681em 0 0;content:"L"}mjx-container[jax=CHTML]{line-height:0}mjx-container [space="1"]{margin-left:.111em}mjx-container [space="2"]{margin-left:.167em}mjx-container [space="3"]{margin-left:.222em}mjx-container [space="4"]{margin-left:.278em}mjx-container [space="5"]{margin-left:.333em}mjx-container [rspace="1"]{margin-right:.111em}mjx-container [rspace="2"]{margin-right:.167em}mjx-container [rspace="3"]{margin-right:.222em}mjx-container [rspace="4"]{margin-right:.278em}mjx-container [rspace="5"]{margin-right:.333em}mjx-container [size="s"]{font-size:70.7%}mjx-container [size=ss]{font-size:50%}mjx-container [size=Tn]{font-size:60%}mjx-container [size=sm]{font-size:85%}mjx-container [size=lg]{font-size:120%}mjx-container [size=Lg]{font-size:144%}mjx-container [size=LG]{font-size:173%}mjx-container [size=hg]{font-size:207%}mjx-container [size=HG]{font-size:249%}mjx-container [width=full]{width:100%}mjx-box{display:inline-block}mjx-block{display:block}mjx-itable{display:inline-table}mjx-row{display:table-row}mjx-row>*{display:table-cell}mjx-mtext{display:inline-block}mjx-mstyle{display:inline-block}mjx-merror{display:inline-block;color:red;background-color:#ff0}mjx-mphantom{visibility:hidden}mjx-assistive-mml{top:0;left:0;clip:rect(1px,1px,1px,1px);user-select:none;position:absolute!important;padding:1px 0 0!important;border:0!important;display:block!important;width:auto!important;overflow:hidden!important}mjx-assistive-mml[display=block]{width:100%!important}mjx-math{display:inline-block;text-align:left;line-height:0;text-indent:0;font-style:normal;font-weight:400;font-size:100%;letter-spacing:normal;border-collapse:collapse;overflow-wrap:normal;word-spacing:normal;white-space:nowrap;direction:ltr;padding:1px 0}mjx-container[jax=CHTML][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=CHTML][display=true][width=full]{display:flex}mjx-container[jax=CHTML][display=true] mjx-math{padding:0}mjx-container[jax=CHTML][justify=left]{text-align:left}mjx-container[jax=CHTML][justify=right]{text-align:right}mjx-mi{display:inline-block;text-align:left}mjx-c{display:inline-block}mjx-utext{display:inline-block;padding:.75em 0 .2em}mjx-mn{display:inline-block;text-align:left}mjx-mo{display:inline-block;text-align:left}mjx-stretchy-h{display:inline-table;width:100%}mjx-stretchy-h>*{display:table-cell;width:0}mjx-stretchy-h>*>mjx-c{display:inline-block;transform:scaleX(1)}mjx-stretchy-h>*>mjx-c::before{display:inline-block;width:initial}mjx-stretchy-h>mjx-ext{overflow:clip visible;width:100%}mjx-stretchy-h>mjx-ext>mjx-c::before{transform:scaleX(500)}mjx-stretchy-h>mjx-ext>mjx-c{width:0}mjx-stretchy-h>mjx-beg>mjx-c{margin-right:-.1em}mjx-stretchy-h>mjx-end>mjx-c{margin-left:-.1em}mjx-stretchy-v{display:inline-block}mjx-stretchy-v>*{display:block}mjx-stretchy-v>mjx-beg{height:0}mjx-stretchy-v>mjx-end>mjx-c{display:block}mjx-stretchy-v>*>mjx-c{transform:scaleY(1);transform-origin:left center;overflow:hidden}mjx-stretchy-v>mjx-ext{display:block;height:100%;box-sizing:border-box;border:0 solid transparent;overflow:visible clip}mjx-stretchy-v>mjx-ext>mjx-c::before{width:initial;box-sizing:border-box}mjx-stretchy-v>mjx-ext>mjx-c{transform:scaleY(500) translateY(.075em);overflow:visible}mjx-mark{display:inline-block;height:0}mjx-c::before{display:block;width:0}.MJX-TEX{font-family:MJXZERO,MJXTEX}.TEX-B{font-family:MJXZERO,MJXTEX-B}.TEX-I{font-family:MJXZERO,MJXTEX-I}.TEX-MI{font-family:MJXZERO,MJXTEX-MI}.TEX-BI{font-family:MJXZERO,MJXTEX-BI}.TEX-S1{font-family:MJXZERO,MJXTEX-S1}.TEX-S2{font-family:MJXZERO,MJXTEX-S2}.TEX-S3{font-family:MJXZERO,MJXTEX-S3}.TEX-S4{font-family:MJXZERO,MJXTEX-S4}.TEX-A{font-family:MJXZERO,MJXTEX-A}.TEX-C{font-family:MJXZERO,MJXTEX-C}.TEX-CB{font-family:MJXZERO,MJXTEX-CB}.TEX-FR{font-family:MJXZERO,MJXTEX-FR}.TEX-FRB{font-family:MJXZERO,MJXTEX-FRB}.TEX-SS{font-family:MJXZERO,MJXTEX-SS}.TEX-SSB{font-family:MJXZERO,MJXTEX-SSB}.TEX-SSI{font-family:MJXZERO,MJXTEX-SSI}.TEX-SC{font-family:MJXZERO,MJXTEX-SC}.TEX-T{font-family:MJXZERO,MJXTEX-T}.TEX-V{font-family:MJXZERO,MJXTEX-V}.TEX-VB{font-family:MJXZERO,MJXTEX-VB}mjx-stretchy-h mjx-c,mjx-stretchy-v mjx-c{font-family:MJXZERO,MJXTEX-S1,MJXTEX-S4,MJXTEX,MJXTEX-A!important}@font-face{font-family:MJXZERO;src:url("lib/fonts/mathjax_zero.woff") format("woff")}@font-face{font-family:MJXTEX;src:url("lib/fonts/mathjax_main-regular.woff") format("woff")}@font-face{font-family:MJXTEX-B;src:url("lib/fonts/mathjax_main-bold.woff") format("woff")}@font-face{font-family:MJXTEX-I;src:url("lib/fonts/mathjax_math-italic.woff") format("woff")}@font-face{font-family:MJXTEX-MI;src:url("lib/fonts/mathjax_main-italic.woff") format("woff")}@font-face{font-family:MJXTEX-BI;src:url("lib/fonts/mathjax_math-bolditalic.woff") format("woff")}@font-face{font-family:MJXTEX-S1;src:url("lib/fonts/mathjax_size1-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S2;src:url("lib/fonts/mathjax_size2-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S3;src:url("lib/fonts/mathjax_size3-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S4;src:url("lib/fonts/mathjax_size4-regular.woff") format("woff")}@font-face{font-family:MJXTEX-A;src:url("lib/fonts/mathjax_ams-regular.woff") format("woff")}@font-face{font-family:MJXTEX-C;src:url("lib/fonts/mathjax_calligraphic-regular.woff") format("woff")}@font-face{font-family:MJXTEX-CB;src:url("lib/fonts/mathjax_calligraphic-bold.woff") format("woff")}@font-face{font-family:MJXTEX-FR;src:url("lib/fonts/mathjax_fraktur-regular.woff") format("woff")}@font-face{font-family:MJXTEX-FRB;src:url("lib/fonts/mathjax_fraktur-bold.woff") format("woff")}@font-face{font-family:MJXTEX-SS;src:url("lib/fonts/mathjax_sansserif-regular.woff") format("woff")}@font-face{font-family:MJXTEX-SSB;src:url("lib/fonts/mathjax_sansserif-bold.woff") format("woff")}@font-face{font-family:MJXTEX-SSI;src:url("lib/fonts/mathjax_sansserif-italic.woff") format("woff")}@font-face{font-family:MJXTEX-SC;src:url("lib/fonts/mathjax_script-regular.woff") format("woff")}@font-face{font-family:MJXTEX-T;src:url("lib/fonts/mathjax_typewriter-regular.woff") format("woff")}@font-face{font-family:MJXTEX-V;src:url("lib/fonts/mathjax_vector-regular.woff") format("woff")}@font-face{font-family:MJXTEX-VB;src:url("lib/fonts/mathjax_vector-bold.woff") format("woff")}mjx-c.mjx-c1D464.TEX-I::before{padding:.443em .716em .011em 0;content:"w"}mjx-c.mjx-c31::before{padding:.666em .5em 0 0;content:"1"}mjx-c.mjx-c1D465.TEX-I::before{padding:.442em .572em .011em 0;content:"x"}mjx-c.mjx-c2B::before{padding:.583em .778em .082em 0;content:"+"}mjx-c.mjx-c32::before{padding:.666em .5em 0 0;content:"2"}mjx-c.mjx-c2E::before{padding:.12em .278em 0 0;content:"."}mjx-c.mjx-c1D45B.TEX-I::before{padding:.442em .6em .011em 0;content:"n"}mjx-c.mjx-c2217::before{padding:.465em .5em 0 0;content:"∗"}mjx-c.mjx-c2212::before{padding:.583em .778em .082em 0;content:"−"}mjx-c.mjx-c1D44F.TEX-I::before{padding:.694em .429em .011em 0;content:"b"}mjx-c.mjx-c3D::before{padding:.583em .778em .082em 0;content:"="}mjx-c.mjx-c30::before{padding:.666em .5em .022em 0;content:"0"}</style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="<strong>CLIP是如何工作的</strong>"><strong>CLIP是如何工作的</strong></h1><div class="el-p"><p dir="auto"><a href="?query=tag:%E9%A1%B9%E7%9B%AE/%E7%9F%A5%E8%AF%86%E5%82%A8%E5%A4%87" class="tag" target="_blank" rel="noopener nofollow">#项目/知识储备</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/493489688" rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/493489688" target="_blank">神器CLIP：连接文本和图像，打造可迁移的视觉模型 - 知乎 (zhihu.com)</a><br>
<strong>clip模型代码</strong><br>
<a data-tooltip-position="top" aria-label="https://github.com/OpenAI/CLIP" rel="noopener nofollow" class="external-link" href="https://github.com/OpenAI/CLIP" target="_blank">openai/CLIP: CLIP (Contrastive Language-Image Pretraining), Predict the most relevant text snippet given an image --- openai/CLIP： CLIP （Contrastive Language-Image Pretraining），预测给定图像的最相关的文本片段 (github.com)</a></p></div><div class="el-h1 heading-wrapper"><div class="heading-children"><div class="el-p"><p dir="auto">CLIP的英文全称是<strong>Contrastive Language-Image Pre-training</strong>，即<strong>一种基于对比文本-图像对的预训练方法或者模型</strong>。CLIP是一种基于对比学习的多模态模型，与CV中的一些对比学习方法如moco和simclr不同的是，CLIP的训练数据是文本-图像对：一张图像和它对应的文本描述，这里希望通过对比学习，模型能够学习到文本-图像对的匹配关系。如下图所示，CLIP包括两个模型：<strong>Text Encoder</strong>和<strong>Image Encoder</strong>，其中Text Encoder用来提取文本的特征，可以采用NLP中常用的text transformer模型；而Image Encoder用来提取图像的特征，可以采用常用CNN模型或者vision transformer。</p></div><div class="el-p"><p dir="auto"><img src="https://pic2.zhimg.com/80/v2-b86361b47d4db80258439b8ad33bdf8d_1440w.webp" referrerpolicy="no-referrer"></p></div><div class="el-p"><p dir="auto">这里对提取的文本特征和图像特征进行对比学习。对于一个包含n个文本-图像对的训练batch，将n个文本特征和n个图像特征两两组合，CLIP模型会预测出n^2个可能的文本-图像对的相似度，这里的相似度直接<strong>计算文本特征和图像特征的<a data-href="余弦相似性" href="project/知识储备/余弦相似性.html" class="internal-link" target="_self" rel="noopener nofollow">余弦相似性</a>（cosine similarity）</strong>，即上图所示的矩阵。这里共有n个正样本，即真正属于一对的文本和图像（矩阵中的对角线元素），而剩余的n^2−n个文本-图像对为负样本，那么CLIP的训练目标就是最大n个正样本的相似度，<em>同时最小化n^2−n个负样本的相似度</em>，对应的伪代码实现如下所示：<br>
<em>问题：为什么要最小化负样本相似度呢？</em></p></div><div class="el-pre"><pre class="language-python" tabindex="0"><code data-line="0" class="language-python is-loaded"><span class="token comment"># image_encoder - ResNet or Vision Transformer</span>
<span class="token comment"># text_encoder - CBOW or Text Transformer</span>
<span class="token comment"># I[n, h, w, c] - minibatch of aligned images</span>
<span class="token comment"># T[n, l] - minibatch of aligned texts</span>
<span class="token comment"># W_i[d_i, d_e] - learned proj of image to embed</span>
<span class="token comment"># W_t[d_t, d_e] - learned proj of text to embed</span>
<span class="token comment"># t - learned temperature parameter</span>

<span class="token comment"># 分别提取图像特征和文本特征</span>
I_f <span class="token operator">=</span> image_encoder<span class="token punctuation">(</span>I<span class="token punctuation">)</span> <span class="token comment">#[n, d_i]</span>
T_f <span class="token operator">=</span> text_encoder<span class="token punctuation">(</span>T<span class="token punctuation">)</span> <span class="token comment">#[n, d_t]</span>

<span class="token comment"># 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化</span>
I_e <span class="token operator">=</span> l2_normalize<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>I_f<span class="token punctuation">,</span> W_i<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
T_e <span class="token operator">=</span> l2_normalize<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>T_f<span class="token punctuation">,</span> W_t<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 计算缩放的余弦相似度：[n, n]</span>
logits <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>I_e<span class="token punctuation">,</span> T_e<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>t<span class="token punctuation">)</span>

<span class="token comment"># 对称的对比学习损失：等价于N个类别的cross_entropy_loss</span>
labels <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>n<span class="token punctuation">)</span> <span class="token comment"># 对角线元素的labels</span>
loss_i <span class="token operator">=</span> cross_entropy_loss<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
loss_t <span class="token operator">=</span> cross_entropy_loss<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> <span class="token punctuation">(</span>loss_i <span class="token operator">+</span> loss_t<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span>
</code><button class="copy-code-button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-copy"><rect x="8" y="8" width="14" height="14" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></pre></div><div class="el-p"><p dir="auto">为了训练CLIP，OpenAI从互联网收集了共<strong>4个亿的文本-图像对</strong>，论文称之为<strong>WebImageText</strong>，如果按照文本的单词量，它和训练GPT-2的WebText规模类似，如果从数量上对比的话，它还比谷歌的JFT-300M数据集多一个亿，所以说这是一个很大规模的数据集。CLIP虽然是多模态模型，但它主要是用来<strong>训练可迁移的视觉模型</strong>。论文中Text Encoder固定选择一个包含63M参数的text transformer模型，而Image Encoder采用了两种的不同的架构，一是常用的CNN架构ResNet，二是基于transformer的ViT，其中ResNet包含5个不同大小的模型：<strong>ResNet50</strong>，<strong>ResNet101</strong>，<strong>RN50x4</strong>，<strong>RN50x16</strong>和<strong>RNx64</strong>（后面三个模型是按照EfficientNet缩放规则对ResNet分别增大4x，16x和64x得到），而ViT选择3个不同大小的模型：<strong>ViT-B/32</strong>，<strong>ViT-B/16</strong>和<strong>ViT-L/14</strong>。所有的模型都训练32个epochs，采用AdamW优化器，而且训练过程采用了一个<strong>较大的batch size：32768</strong>。由于数据量较大，最大的ResNet模型RN50x64需要在592个V100卡上训练18天，而最大ViT模型ViT-L/14需要在256张V100卡上训练12天，可见要训练CLIP需要耗费多大的资源。对于ViT-L/14，还在336的分辨率下额外finetune了一个epoch来增强性能，论文发现这个模型效果最好，记为<strong>ViT-L/14@336</strong>，论文中进行对比实验的CLIP模型也采用这个。</p></div></div></div><div class="el-h1 heading-wrapper"><h1 data-heading="**如何用CLIP实现[[zero-shot]]分类**" dir="auto" class="heading" id="**如何用CLIP实现[[zero-shot]]分类**"><strong>如何用CLIP实现<a data-href="zero-shot" href="project/知识储备/zero-shot.html" class="internal-link" target="_self" rel="noopener nofollow">zero-shot</a>分类</strong></h1><div class="heading-children"><div class="el-p"><p dir="auto">上面我们介绍了CLIP的原理，可以看到训练后的CLIP其实是两个模型，除了视觉模型外还有一个文本模型，那么如何对预训练好的视觉模型进行迁移呢？<strong>与CV中常用的先预训练然后微调不同，CLIP可以直接实现zero-shot的图像分类，即不需要任何训练数据，就能在某个具体下游任务上实现分类，</strong>这也是CLIP亮点和强大之处。用CLIP实现zero-shot分类很简单，只需要简单的两步：</p></div><div class="el-ol"><ol>
<li data-line="0" dir="auto">根据任务的分类标签构建每个类别的描述文本：<code>A photo of {label}</code>，然后将这些文本送入Text Encoder得到对应的文本特征，如果类别数目为n，那么将得到n个文本特征；</li>
<li data-line="1" dir="auto">将要预测的图像送入Image Encoder得到图像特征，然后与n个文本特征计算缩放的余弦相似度（和训练过程一致），然后选择相似度最大的文本对应的类别作为图像分类预测结果，进一步地，可以将这些相似度看成logits，送入softmax后可以到每个类别的预测概率。</li>
</ol></div><div class="el-p"><p dir="auto"><img src="https://pic4.zhimg.com/80/v2-acd4b008007ca7de78bdab1c9042bbcb_1440w.webp" referrerpolicy="no-referrer"></p></div><div class="el-p"><p dir="auto">可以看到，我们是利用CLIP的多模态特性为具体的任务<strong>构建了动态的分类器</strong>，<strong>其中Text Encoder提取的文本特征可以看成分类器的weights，而Image Encoder提取的图像特征是分类器的输入</strong>。这里我们给出了一个基于CLIP的一个实例（参考官方<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb" target="_blank">notebook</a>），这里任务共有6个类别："dog", "cat", "bird", "person", "mushroom", "cup"，首先我们创建文本描述，然后提取文本特征：</p></div><div class="el-pre"><pre class="language-python" tabindex="0"><code data-line="0" class="language-python is-loaded"><span class="token comment"># 首先生成每个类别的文本描述</span>
labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"dog"</span><span class="token punctuation">,</span> <span class="token string">"cat"</span><span class="token punctuation">,</span> <span class="token string">"bird"</span><span class="token punctuation">,</span> <span class="token string">"person"</span><span class="token punctuation">,</span> <span class="token string">"mushroom"</span><span class="token punctuation">,</span> <span class="token string">"cup"</span><span class="token punctuation">]</span>
text_descriptions <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f"A photo of a </span><span class="token interpolation"><span class="token punctuation">{</span>label<span class="token punctuation">}</span></span><span class="token string">"</span></span> <span class="token keyword">for</span> label <span class="token keyword">in</span> labels<span class="token punctuation">]</span>
text_tokens <span class="token operator">=</span> clip<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>text_descriptions<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 提取文本特征</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    text_features <span class="token operator">=</span> model<span class="token punctuation">.</span>encode_text<span class="token punctuation">(</span>text_tokens<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    text_features <span class="token operator">/=</span> text_features<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code><button class="copy-code-button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-copy"><rect x="8" y="8" width="14" height="14" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></pre></div><div class="el-p"><p dir="auto">然后我们读取要预测的图像，输入Image Encoder提取图像特征，并计算与文本特征的余弦相似度：</p></div><div class="el-pre"><pre class="language-python" tabindex="0"><code data-line="0" class="language-python is-loaded"><span class="token comment"># 读取图像</span>
original_images <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
images <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
texts <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> label <span class="token keyword">in</span> labels<span class="token punctuation">:</span>
    image_file <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">"images"</span><span class="token punctuation">,</span> label<span class="token operator">+</span><span class="token string">".jpg"</span><span class="token punctuation">)</span>
    name <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>basename<span class="token punctuation">(</span>image_file<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'.'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    image <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>image_file<span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">"RGB"</span><span class="token punctuation">)</span>
    original_images<span class="token punctuation">.</span>append<span class="token punctuation">(</span>image<span class="token punctuation">)</span>
    images<span class="token punctuation">.</span>append<span class="token punctuation">(</span>preprocess<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">)</span>
    texts<span class="token punctuation">.</span>append<span class="token punctuation">(</span>name<span class="token punctuation">)</span>

image_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>images<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 提取图像特征  </span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    image_features <span class="token operator">=</span> model<span class="token punctuation">.</span>encode_image<span class="token punctuation">(</span>image_input<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    image_features <span class="token operator">/=</span> image_features<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 计算余弦相似度（未缩放）</span>
similarity <span class="token operator">=</span> text_features<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span> @ image_features<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T
</code><button class="copy-code-button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-copy"><rect x="8" y="8" width="14" height="14" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></pre></div><div class="el-p"><p dir="auto">相似度如下所示，可以看到对于要预测的6个图像，按照最大相似度，其均能匹配到正确的文本标签：</p></div><div class="el-p"><p dir="auto"><img src="https://pic4.zhimg.com/80/v2-b16c535d2c33028e33c59d041d4aa8cf_1440w.webp" referrerpolicy="no-referrer"></p></div><div class="el-p"><p dir="auto">进一步地，我们也可以对得到的余弦相似度计算softmax，得到每个预测类别的概率值，注意这里要对相似度进行缩放：</p></div><div class="el-pre"><pre class="language-python" tabindex="0"><code data-line="0" class="language-python is-loaded">logit_scale <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>model<span class="token punctuation">.</span>logit_scale<span class="token punctuation">.</span>data<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
text_probs <span class="token operator">=</span> <span class="token punctuation">(</span>logit_scale <span class="token operator">*</span> image_features @ text_features<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
top_probs<span class="token punctuation">,</span> top_labels <span class="token operator">=</span> text_probs<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code><button class="copy-code-button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-copy"><rect x="8" y="8" width="14" height="14" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></pre></div><div class="el-p"><p dir="auto">得到的预测概率如下所示，可以看到6个图像，CLIP模型均能够以绝对的置信度给出正确的分类结果：</p></div><div class="el-p"><p dir="auto"><img src="https://pic2.zhimg.com/80/v2-00749f75a141a99c98207fab0c91c2ad_1440w.webp" referrerpolicy="no-referrer"></p></div><div class="el-p"><p dir="auto">使用CLIP进行zero-shot分类，另外一个比较重要的地方是<strong>文本描述的生成</strong>，上面的例子我们采用<code>A photo of {label}</code>，但其实也有其它选择，比如我们直接用类别标签，这其实属于最近NLP领域比较火的一个研究：prompt learning或者prompt engineering，具体可以见这篇综述论文：<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2107.13586" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2107.13586" target="_blank">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</a>，简单来说，prompt learning的核心是通过构建合适prompt（提示）来使预训练模型能够直接应用到下游任务，这和之前的预训练+微调属于不同的范式。论文也说了，如果我们直接采用类别标签作为文本描述，那么很多文本就是一个单词，缺少具体的上下文，而且也和CLIP的训练数据不太一致，效果上会不如采用<code>A photo of {label}</code>（ImageNet数据集上可以提升1.3%）。论文也实验了采用80个不同的prompt来进行集成，发现在ImageNet数据集上能带来3.5%的提升，具体见CLIP公开的<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb" target="_blank">notebook</a>。下图对比了基于ResNet的CLIP模型直接采用类别名与进行prompt engineering和ensembling的效果对比：</p></div><div class="el-p"><p dir="auto"><img src="https://pic2.zhimg.com/80/v2-cbebe6dfb596bf0c93a4c22c441f17cd_1440w.webp" referrerpolicy="no-referrer"></p></div><div class="el-p"><p dir="auto">上面我们介绍了如何用CLIP实现zero-shot分类，下面将简单介绍CLIP与其它方法的效果对比，这个也是论文中篇幅最多的内容。首先是CLIP和17年的一篇工作<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.09161" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.09161" target="_blank">Learning Visual N-Grams from Web Data</a>的在3个分类数据集上zero-shot效果对比，如下表所示，可以看到CLIP模型在效果上远远超过之前的模型，<strong>其中在ImageNet数据集可以达到76.2，这和全监督的ResNet50效果相当</strong>，不用任何训练数据就能达到这个效果是相当惊艳的。</p></div><div class="el-p"><p dir="auto"><img src="https://pic1.zhimg.com/80/v2-d3c1c6676c28f75d26029c0cde43ed60_1440w.webp" referrerpolicy="no-referrer"></p></div><div class="el-p"><p dir="auto">更进一步地，论文还对比了zero-shot CLIP和ResNet50 linear probing（ImageNet数据上预训练，在加上线性分类层进行finetune）在27个数据集上表现，如下图所示，其中在16个数据集上CLIP可以超过ResNet50。但是<strong>在一些特别的，复杂的或者抽象的数据集上CLIP表现较差</strong>，比如卫星图像分类，淋巴结转移检测，在合成场景中计数等，CLIP的效果不如全监督的ResNet50，这说明CLIP并不是万能的，还是有改进的空间。如果认真看下图的话，CLIP表现较差的竟然还有MNIST数据集，分类准确度只有88%，这是不可思议的，因为这个任务太简单了，通过对CLIP训练数据进行分析，作者发现4亿的训练数据中基本上没有和MNIST比较相似的数据，所以这对CLIP来说就属于<strong>域外数据</strong>了，表现较差就比较容易理解了。这也表明：<strong>CLIP依然无法解决域外泛化这个深度学习难题。</strong></p></div><div class="el-p"><p dir="auto"><img src="https://pic1.zhimg.com/80/v2-dc8ba977117fdec5fd2f79961d7110d4_1440w.webp" referrerpolicy="no-referrer"></p></div><div class="el-p"><p dir="auto">除了zero-shot对比，论文还对比few-shot性能，即只用少量的样本来微调模型，这里对比了3个模型：在ImageNet21K上训练的BiT-M ResNet-152x2，基于SimCLRv2训练的ResNet50，以及有监督训练的ResNet50。可以看到CLIP的zero-shot和最好的模型（BiT-M）在16-shot下的性能相当，而CLIP在16-shot下效果有进一步的提升。另外一个比较有意思的结果是：虽然CLIP在few-shot实验中随着样本量增加性能有提升，但是1-shot和2-shot性能比zero-shot还差，这个作者认为主要是CLIP的训练和常规的有监督训练存在一定的差异造成的。</p></div><div class="el-p"><p dir="auto"><img src="https://pic3.zhimg.com/80/v2-4d122ef5767f0453121698d4bad08036_1440w.webp" referrerpolicy="no-referrer"></p></div><div class="el-p"><p dir="auto">除此之外，论文还进行了<strong>表征学习</strong>（<strong>representation Learning</strong>）实验，即自监督学习中常用的<strong>linear probe</strong>：用训练好的模型先提取特征，然后用一个线性分类器来有监督训练。下图为不同模型在27个数据集上的average linear probe score对比，可以看到CLIP模型在性能上超过其它模型，而且计算更高效：</p></div><div class="el-p"><p dir="auto"><img src="https://pic1.zhimg.com/80/v2-42d05e933b46aebf16ba11e2cfce6154_1440w.webp" referrerpolicy="no-referrer"></p></div><div class="el-p"><p dir="auto">另外，论文还发现CLIP在自然分布漂移上表现更鲁棒，比如CLIP和基于ImageNet上有监督训练的ResNet101在ImageNet验证集都能达到76.2%，但是在ImageNetV2数据集上，CLIP要超过ResNet101。在另外的4个分布漂移的数据集上，ResNet101性能下降得比较厉害，但是CLIP能依然保持较大的准确度，比如在ImageNet-A数据集上，ResNet101性能只有2.7%，而CLIP能达到77.1%。</p></div><div class="el-p"><p dir="auto"><img src="https://pic2.zhimg.com/80/v2-7798ab2d513a0ab79117adcf74d8ec05_1440w.webp" referrerpolicy="no-referrer"></p></div><div class="el-p"><p dir="auto">CLIP能实现这么好的zero-shot性能，大家很可能质疑CLIP的训练数据集可能包含一些测试数据集中的样例，即所谓的数据泄漏。关于这点，论文也采用一个重复检测器对评测的数据集重合做了检查，发现重合率的中位数为2.2%，而平均值在3.2%，去重前后大部分数据集的性能没有太大的变化，如下所示：</p></div><div class="el-p"><p dir="auto"><img src="https://pic1.zhimg.com/80/v2-dcbbf5bc5e3f0e3ff60155547fd134b8_1440w.webp" referrerpolicy="no-referrer"></p></div><div class="el-p"><p dir="auto">论文的最后也对CLIP的局限性做了讨论，这里简单总结其中比较重要的几点：</p></div><div class="el-ul"><ul>
<li data-line="0" dir="auto">CLIP的zero-shot性能虽然和有监督的ResNet50相当，但是还不如<a data-href="SOTA" href="project/知识储备/sota.html" class="internal-link" target="_self" rel="noopener nofollow">SOTA</a>，作者估计要达到SOTA的效果，CLIP还需要增加1000x的计算量，这是难以想象的；</li>
<li data-line="1" dir="auto">CLIP的zero-shot在某些数据集上表现较差，如细粒度分类，抽象任务等；</li>
<li data-line="2" dir="auto">CLIP在自然分布漂移上表现鲁棒，但是依然存在域外泛化问题，即如果测试数据集的分布和训练集相差较大，CLIP会表现较差；</li>
<li data-line="3" dir="auto">CLIP并没有解决深度学习的数据效率低下难题，训练CLIP需要大量的数据；</li>
</ul></div><div class="mod-footer mod-ui"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="project\知识储备\clip.html#<strong>CLIP是如何工作的</strong>"><div class="tree-item-contents heading-link" heading-name="CLIP是如何工作的"><span class="tree-item-title">CLIP是如何工作的</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="project\知识储备\clip.html#**如何用CLIP实现[[zero-shot]]分类**"><div class="tree-item-contents heading-link" heading-name="**如何用CLIP实现[[zero-shot]]分类**"><span class="tree-item-title"><strong>如何用CLIP实现<a data-href="zero-shot" href="zero-shot" class="internal-link" target="_blank" rel="noopener nofollow">zero-shot</a>分类</strong></span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>