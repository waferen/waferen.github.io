<!DOCTYPE html> <html><head>
		<title>大创中期答辩</title>
		<base href="..\../">
		<meta id="root-path" root-path="..\../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="note - 大创中期答辩">
		<meta property="og:title" content="大创中期答辩">
		<meta property="og:description" content="note - 大创中期答辩">
		<meta property="og:type" content="website">
		<meta property="og:url" content="project/大创/大创中期答辩.html">
		<meta property="og:image" content="undefined">
		<meta property="og:site_name" content="note">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="stylesheet" href="lib/styles/theme.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager theme-dark show-inline-title show-ribbon"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles">mjx-container[jax=CHTML]{line-height:0}mjx-container [space="1"]{margin-left:.111em}mjx-container [space="2"]{margin-left:.167em}mjx-container [space="3"]{margin-left:.222em}mjx-container [space="4"]{margin-left:.278em}mjx-container [space="5"]{margin-left:.333em}mjx-container [rspace="1"]{margin-right:.111em}mjx-container [rspace="2"]{margin-right:.167em}mjx-container [rspace="3"]{margin-right:.222em}mjx-container [rspace="4"]{margin-right:.278em}mjx-container [rspace="5"]{margin-right:.333em}mjx-container [size="s"]{font-size:70.7%}mjx-container [size=ss]{font-size:50%}mjx-container [size=Tn]{font-size:60%}mjx-container [size=sm]{font-size:85%}mjx-container [size=lg]{font-size:120%}mjx-container [size=Lg]{font-size:144%}mjx-container [size=LG]{font-size:173%}mjx-container [size=hg]{font-size:207%}mjx-container [size=HG]{font-size:249%}mjx-container [width=full]{width:100%}mjx-box{display:inline-block}mjx-block{display:block}mjx-itable{display:inline-table}mjx-row{display:table-row}mjx-row>*{display:table-cell}mjx-mtext{display:inline-block}mjx-mstyle{display:inline-block}mjx-merror{display:inline-block;color:red;background-color:#ff0}mjx-mphantom{visibility:hidden}mjx-assistive-mml{top:0;left:0;clip:rect(1px,1px,1px,1px);user-select:none;position:absolute!important;padding:1px 0 0!important;border:0!important;display:block!important;width:auto!important;overflow:hidden!important}mjx-assistive-mml[display=block]{width:100%!important}mjx-math{display:inline-block;text-align:left;line-height:0;text-indent:0;font-style:normal;font-weight:400;font-size:100%;letter-spacing:normal;border-collapse:collapse;overflow-wrap:normal;word-spacing:normal;white-space:nowrap;direction:ltr;padding:1px 0}mjx-container[jax=CHTML][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=CHTML][display=true][width=full]{display:flex}mjx-container[jax=CHTML][display=true] mjx-math{padding:0}mjx-container[jax=CHTML][justify=left]{text-align:left}mjx-container[jax=CHTML][justify=right]{text-align:right}mjx-mi{display:inline-block;text-align:left}mjx-c{display:inline-block}mjx-utext{display:inline-block;padding:.75em 0 .2em}mjx-mn{display:inline-block;text-align:left}mjx-mo{display:inline-block;text-align:left}mjx-stretchy-h{display:inline-table;width:100%}mjx-stretchy-h>*{display:table-cell;width:0}mjx-stretchy-h>*>mjx-c{display:inline-block;transform:scaleX(1)}mjx-stretchy-h>*>mjx-c::before{display:inline-block;width:initial}mjx-stretchy-h>mjx-ext{overflow:clip visible;width:100%}mjx-stretchy-h>mjx-ext>mjx-c::before{transform:scaleX(500)}mjx-stretchy-h>mjx-ext>mjx-c{width:0}mjx-stretchy-h>mjx-beg>mjx-c{margin-right:-.1em}mjx-stretchy-h>mjx-end>mjx-c{margin-left:-.1em}mjx-stretchy-v{display:inline-block}mjx-stretchy-v>*{display:block}mjx-stretchy-v>mjx-beg{height:0}mjx-stretchy-v>mjx-end>mjx-c{display:block}mjx-stretchy-v>*>mjx-c{transform:scaleY(1);transform-origin:left center;overflow:hidden}mjx-stretchy-v>mjx-ext{display:block;height:100%;box-sizing:border-box;border:0 solid transparent;overflow:visible clip}mjx-stretchy-v>mjx-ext>mjx-c::before{width:initial;box-sizing:border-box}mjx-stretchy-v>mjx-ext>mjx-c{transform:scaleY(500) translateY(.075em);overflow:visible}mjx-mark{display:inline-block;height:0}mjx-c::before{display:block;width:0}.MJX-TEX{font-family:MJXZERO,MJXTEX}.TEX-B{font-family:MJXZERO,MJXTEX-B}.TEX-I{font-family:MJXZERO,MJXTEX-I}.TEX-MI{font-family:MJXZERO,MJXTEX-MI}.TEX-BI{font-family:MJXZERO,MJXTEX-BI}.TEX-S1{font-family:MJXZERO,MJXTEX-S1}.TEX-S2{font-family:MJXZERO,MJXTEX-S2}.TEX-S3{font-family:MJXZERO,MJXTEX-S3}.TEX-S4{font-family:MJXZERO,MJXTEX-S4}.TEX-A{font-family:MJXZERO,MJXTEX-A}.TEX-C{font-family:MJXZERO,MJXTEX-C}.TEX-CB{font-family:MJXZERO,MJXTEX-CB}.TEX-FR{font-family:MJXZERO,MJXTEX-FR}.TEX-FRB{font-family:MJXZERO,MJXTEX-FRB}.TEX-SS{font-family:MJXZERO,MJXTEX-SS}.TEX-SSB{font-family:MJXZERO,MJXTEX-SSB}.TEX-SSI{font-family:MJXZERO,MJXTEX-SSI}.TEX-SC{font-family:MJXZERO,MJXTEX-SC}.TEX-T{font-family:MJXZERO,MJXTEX-T}.TEX-V{font-family:MJXZERO,MJXTEX-V}.TEX-VB{font-family:MJXZERO,MJXTEX-VB}mjx-stretchy-h mjx-c,mjx-stretchy-v mjx-c{font-family:MJXZERO,MJXTEX-S1,MJXTEX-S4,MJXTEX,MJXTEX-A!important}@font-face{font-family:MJXZERO;src:url("lib/fonts/mathjax_zero.woff") format("woff")}@font-face{font-family:MJXTEX;src:url("lib/fonts/mathjax_main-regular.woff") format("woff")}@font-face{font-family:MJXTEX-B;src:url("lib/fonts/mathjax_main-bold.woff") format("woff")}@font-face{font-family:MJXTEX-I;src:url("lib/fonts/mathjax_math-italic.woff") format("woff")}@font-face{font-family:MJXTEX-MI;src:url("lib/fonts/mathjax_main-italic.woff") format("woff")}@font-face{font-family:MJXTEX-BI;src:url("lib/fonts/mathjax_math-bolditalic.woff") format("woff")}@font-face{font-family:MJXTEX-S1;src:url("lib/fonts/mathjax_size1-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S2;src:url("lib/fonts/mathjax_size2-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S3;src:url("lib/fonts/mathjax_size3-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S4;src:url("lib/fonts/mathjax_size4-regular.woff") format("woff")}@font-face{font-family:MJXTEX-A;src:url("lib/fonts/mathjax_ams-regular.woff") format("woff")}@font-face{font-family:MJXTEX-C;src:url("lib/fonts/mathjax_calligraphic-regular.woff") format("woff")}@font-face{font-family:MJXTEX-CB;src:url("lib/fonts/mathjax_calligraphic-bold.woff") format("woff")}@font-face{font-family:MJXTEX-FR;src:url("lib/fonts/mathjax_fraktur-regular.woff") format("woff")}@font-face{font-family:MJXTEX-FRB;src:url("lib/fonts/mathjax_fraktur-bold.woff") format("woff")}@font-face{font-family:MJXTEX-SS;src:url("lib/fonts/mathjax_sansserif-regular.woff") format("woff")}@font-face{font-family:MJXTEX-SSB;src:url("lib/fonts/mathjax_sansserif-bold.woff") format("woff")}@font-face{font-family:MJXTEX-SSI;src:url("lib/fonts/mathjax_sansserif-italic.woff") format("woff")}@font-face{font-family:MJXTEX-SC;src:url("lib/fonts/mathjax_script-regular.woff") format("woff")}@font-face{font-family:MJXTEX-T;src:url("lib/fonts/mathjax_typewriter-regular.woff") format("woff")}@font-face{font-family:MJXTEX-V;src:url("lib/fonts/mathjax_vector-regular.woff") format("woff")}@font-face{font-family:MJXTEX-VB;src:url("lib/fonts/mathjax_vector-bold.woff") format("woff")}mjx-c.mjx-c1D464.TEX-I::before{padding:.443em .716em .011em 0;content:"w"}mjx-c.mjx-c31::before{padding:.666em .5em 0 0;content:"1"}mjx-c.mjx-c1D465.TEX-I::before{padding:.442em .572em .011em 0;content:"x"}mjx-c.mjx-c2B::before{padding:.583em .778em .082em 0;content:"+"}mjx-c.mjx-c32::before{padding:.666em .5em 0 0;content:"2"}mjx-c.mjx-c2E::before{padding:.12em .278em 0 0;content:"."}mjx-c.mjx-c1D45B.TEX-I::before{padding:.442em .6em .011em 0;content:"n"}mjx-c.mjx-c2217::before{padding:.465em .5em 0 0;content:"∗"}mjx-c.mjx-c2212::before{padding:.583em .778em .082em 0;content:"−"}mjx-c.mjx-c1D44F.TEX-I::before{padding:.694em .429em .011em 0;content:"b"}mjx-c.mjx-c3D::before{padding:.583em .778em .082em 0;content:"="}mjx-c.mjx-c30::before{padding:.666em .5em .022em 0;content:"0"}</style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="大创中期答辩">大创中期答辩</h1><div class="el-h3 heading-wrapper"><h3 data-heading="大创中期答辩要点" dir="auto" class="heading" id="大创中期答辩要点"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>大创中期答辩要点</h3><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto">成果（要证明我们已经做了很多事了）</li>
</ul></div><div class="el-ol"><ol>
<li data-line="0" dir="auto">手势识别（图片）</li>
<li data-line="1" dir="auto">deepsort（准确度提高）</li>
<li data-line="2" dir="auto">VideoMae 视频特征提取</li>
<li data-line="3" dir="auto">文本特征失败，因为做的是实时监测，测试视频中没有文本特征。</li>
<li data-line="4" dir="auto">正在申请手势软著</li>
<li data-line="5" dir="auto">参与了xx专利（待定）</li>
</ol></div><div class="el-ul"><ul>
<li data-line="0" dir="auto">未来（我们接下来要做的事情）</li>
</ul></div><div class="el-ol"><ol>
<li data-line="0" dir="auto">手势视频多模态融合</li>
</ol></div><div class="el-ul"><ul>
<li data-line="0" dir="auto">目前</li>
</ul></div><div class="el-ol"><ol>
<li data-line="0" dir="auto">我们正在做调研（手势特征提取方法、手势特征与视频特征的对齐融合）</li>
</ol></div></div></div><div class="el-h3 heading-wrapper"><h3 data-heading="调研结果" dir="auto" class="heading" id="调研结果"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>调研结果</h3><div class="heading-children"><div class="el-p"><p dir="auto">关于利用图卷积网络（GCN）进行手势识别的研究，近年来已有一些论文探索了如何将手势数据转化为图结构，并通过图卷积进行有效的分类。以下是一些相关的研究方向和论文：</p></div><div class="el-h5 heading-wrapper"><h5 data-heading="1. **Gesture Recognition Using Graph Convolutional Networks**" dir="auto" class="heading" id="1._**Gesture_Recognition_Using_Graph_Convolutional_Networks**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>1. <strong>Gesture Recognition Using Graph Convolutional Networks</strong></h5><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>论文</strong>: "Graph Convolutional Networks for Hand Gesture Recognition"</li>
<li data-line="1" dir="auto"><strong>概述</strong>: 本文提出了利用图卷积网络（GCN）来进行手势识别的方法。手势图由手指关节的位置和连接关系构成，图卷积网络用于从这些结构化的图中学习特征，进而进行手势识别。</li>
<li data-line="2" dir="auto"><strong>方法</strong>: 手势数据通常由多个关节或关键点的空间坐标表示，利用这些关节之间的空间关系构建图结构。图卷积网络对这些图结构进行处理，挖掘图中的空间信息和局部模式，从而提升手势识别的准确性。</li>
<li data-line="3" dir="auto"><strong>引用</strong>: <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2007.06932" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2007.06932" target="_blank">Paper link</a></li>
</ul></div></div></div><div class="el-h5 heading-wrapper"><h5 data-heading="2. **Hand Gesture Recognition Using Graph Convolutional Networks**" dir="auto" class="heading" id="2._**Hand_Gesture_Recognition_Using_Graph_Convolutional_Networks**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>2. <strong>Hand Gesture Recognition Using Graph Convolutional Networks</strong></h5><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>论文</strong>: "Hand Gesture Recognition with Graph Convolutional Networks"</li>
<li data-line="1" dir="auto"><strong>概述</strong>: 本文提出了一种基于GCN的手势识别方法，重点是通过利用手部骨骼图来构建手势图，GCN则用于学习关节之间的空间关系和运动模式。</li>
<li data-line="2" dir="auto"><strong>方法</strong>: 作者通过构建手部骨骼图，其中每个节点代表手部的一个关节，边则表示关节之间的连接。GCN模型用于捕捉关节之间的依赖关系，并进行多类手势的分类。</li>
<li data-line="3" dir="auto"><strong>引用</strong>: <a data-tooltip-position="top" aria-label="https://ieeexplore.ieee.org/document/9097119" rel="noopener nofollow" class="external-link" href="https://ieeexplore.ieee.org/document/9097119" target="_blank">Paper link</a></li>
</ul></div></div></div><div class="el-h5 heading-wrapper"><h5 data-heading="3. **Learning Hand Pose Estimation and Gesture Recognition with GCN**" dir="auto" class="heading" id="3._**Learning_Hand_Pose_Estimation_and_Gesture_Recognition_with_GCN**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3. <strong>Learning Hand Pose Estimation and Gesture Recognition with GCN</strong></h5><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>论文</strong>: "Learning Hand Pose Estimation and Gesture Recognition with Graph Convolutional Networks"</li>
<li data-line="1" dir="auto"><strong>概述</strong>: 本研究结合了手势识别和手势姿态估计，提出了一种基于图卷积网络的联合学习方法，通过图卷积网络来同时进行手势分类和姿态估计。</li>
<li data-line="2" dir="auto"><strong>方法</strong>: 构建一个动态图模型，其中图中的每个节点代表手的一个关节，每条边则表示关节之间的依赖关系。通过GCN提取图中空间关系来进行更精确的手势识别。</li>
<li data-line="3" dir="auto"><strong>引用</strong>: <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2007.12131" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2007.12131" target="_blank">Paper link</a></li>
</ul></div></div></div><div class="el-h5 heading-wrapper"><h5 data-heading="4. **3D Hand Gesture Recognition Using Graph Convolutional Networks**" dir="auto" class="heading" id="4._**3D_Hand_Gesture_Recognition_Using_Graph_Convolutional_Networks**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4. <strong>3D Hand Gesture Recognition Using Graph Convolutional Networks</strong></h5><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>论文</strong>: "3D Hand Gesture Recognition Using Graph Convolutional Networks"</li>
<li data-line="1" dir="auto"><strong>概述</strong>: 本文提出了一种3D手势识别方法，利用3D空间中的关节坐标通过GCN进行分类，重点解决了传统方法无法有效处理3D空间信息的问题。</li>
<li data-line="2" dir="auto"><strong>方法</strong>: 通过使用3D深度摄像头捕捉手部关节数据，并构建3D图模型，使用GCN来学习手势的空间模式，并进行分类。</li>
<li data-line="3" dir="auto"><strong>引用</strong>: <a data-tooltip-position="top" aria-label="https://ieeexplore.ieee.org/document/9282690" rel="noopener nofollow" class="external-link" href="https://ieeexplore.ieee.org/document/9282690" target="_blank">Paper link</a></li>
</ul></div></div></div><div class="el-h5 heading-wrapper"><h5 data-heading="5. **Dynamic Graph CNN for Gesture Recognition**" dir="auto" class="heading" id="5._**Dynamic_Graph_CNN_for_Gesture_Recognition**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>5. <strong>Dynamic Graph CNN for Gesture Recognition</strong></h5><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>论文</strong>: "Dynamic Graph CNN for Gesture Recognition"</li>
<li data-line="1" dir="auto"><strong>概述</strong>: 这篇论文提出了一种动态图卷积网络（DGCNN）的方法，该方法结合了动态图和卷积神经网络来处理动态手势序列。</li>
<li data-line="2" dir="auto"><strong>方法</strong>: 该方法动态构建手势的图结构，图中的节点代表手部关节，边表示关节之间的关系。动态图卷积网络能捕捉到手势序列的时空信息，从而提升识别精度。</li>
<li data-line="3" dir="auto"><strong>引用</strong>: <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1812.01847" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/1812.01847" target="_blank">Paper link</a></li>
</ul></div></div></div><div class="el-h5 heading-wrapper"><h5 data-heading="6. **Graph Neural Networks for Hand Gesture Recognition in Video**" dir="auto" class="heading" id="6._**Graph_Neural_Networks_for_Hand_Gesture_Recognition_in_Video**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>6. <strong>Graph Neural Networks for Hand Gesture Recognition in Video</strong></h5><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>论文</strong>: "Graph Neural Networks for Hand Gesture Recognition in Video"</li>
<li data-line="1" dir="auto"><strong>概述</strong>: 本文探索了如何利用图神经网络（GNN）进行视频中的手势识别，通过在视频帧中提取手势的图结构，利用GNN进行时序信息的学习。</li>
<li data-line="2" dir="auto"><strong>方法</strong>: 视频中的手势每一帧被表示为一个图结构，通过时序图卷积网络（T-GCN）来捕捉时序和空间上的信息，进行手势的动态识别。</li>
<li data-line="3" dir="auto"><strong>引用</strong>: <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1905.01948" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/1905.01948" target="_blank">Paper link</a></li>
</ul></div></div></div><div class="el-h4 heading-wrapper"><h4 data-heading="结论" dir="auto" class="heading" id="结论"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>结论</h4><div class="heading-children"><div class="el-p"><p dir="auto">这些论文展示了如何将手势数据转化为图结构，并利用图卷积网络（GCN）来进行手势识别。GCN能够有效地捕捉手势数据中的空间和时序关系，从而提升识别精度。可以根据我们的数据类型（如2D关节坐标、3D关节坐标或视频序列）选择合适的方法。</p></div></div></div></div></div><div class="el-h3 heading-wrapper"><h3 data-heading="项目实施情况总结" dir="auto" class="heading" id="项目实施情况总结"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>项目实施情况总结</h3><div class="heading-children"><div class="el-p"><p dir="auto">本项目旨在通过多模态特征提取与融合、行为识别、目标检测等技术，构建一个高效的实验评价系统，特别是在化学实验过程中的动作监测和评估。项目的实施内容涵盖了视频特征提取、手部关节特征提取、多模态特征融合、行为识别与目标检测等多个方面，每个环节都涉及到最新的人工智能技术，特别是深度学习和计算机视觉领域的先进方法。</p></div><div class="el-h4 heading-wrapper"><h4 data-heading="1. 研究内容" dir="auto" class="heading" id="1._研究内容"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>1. 研究内容</h4><div class="heading-children"><div class="el-p"><p dir="auto">本项目的核心研究内容主要包括以下几个部分：</p></div><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>多模态特征提取</strong>：通过提取视频、手势等多种类型的数据特征，为后续的行为识别和目标检测提供有力支持。</li>
<li data-line="1" dir="auto"><strong>多模态特征融合</strong>：将来自不同模态的数据特征进行融合，提升识别准确性和鲁棒性。</li>
<li data-line="2" dir="auto"><strong>行为识别</strong>：基于提取的特征，对实验过程中学生的行为进行实时识别与评估。</li>
<li data-line="3" dir="auto"><strong>目标检测</strong>：实时检测实验过程中的关键物体或设备，确保实验的顺利进行。</li>
<li data-line="4" dir="auto"><strong>评价系统建立</strong>：基于行为识别与目标检测结果，构建一个系统性的实验评价模型，自动评估学生实验操作的质量和准确性。</li>
</ul></div></div></div><div class="el-h4 heading-wrapper"><h4 data-heading="2. 时间安排" dir="auto" class="heading" id="2._时间安排"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>2. 时间安排</h4><div class="heading-children"><div class="el-p"><p dir="auto">本项目的实施过程中，我们为每个阶段设定了明确的时间目标，并进行了灵活调整。最终时间安排如下：</p></div><div class="el-ol"><ol>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>初期准备阶段（1-2个月）</strong>：</p>
<ul>
<li data-line="2" dir="auto">在该阶段，我们主要进行了文献调研，了解相关领域的最新进展，选择合适的模型和算法。</li>
<li data-line="3" dir="auto">进行模型的初步选择与环境搭建。</li>
</ul>
</li>
<li data-line="4" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>中期开发阶段（3-5个月）</strong>：</p>
<ul>
<li data-line="6" dir="auto">开始多模态特征提取和行为识别模块的开发，重点完成视频特征提取与手部关节特征提取的实验。</li>
<li data-line="7" dir="auto">完成MediaPipe Hands模型的集成，实现对手部关节的实时检测。</li>
<li data-line="8" dir="auto">进行多模态数据的融合实验，评估不同特征融合方法的效果。</li>
</ul>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>后期优化阶段（6-8个月）</strong>：</p>
<ul>
<li data-line="11" dir="auto">对模型进行进一步优化，提升识别精度和实时性。</li>
<li data-line="12" dir="auto">开始目标检测模块的开发，确保能够准确检测实验过程中的关键物体。</li>
<li data-line="13" dir="auto">建立实验评价系统，基于多模态特征进行学生实验过程的全面评估。</li>
<li data-line="14" dir="auto">完成系统的整体集成和测试工作，确保系统的稳定性与准确性。</li>
</ul>
</li>
<li data-line="15" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>最终测试与报告阶段（9-10个月）</strong>：</p>
<ul>
<li data-line="17" dir="auto">完成整个系统的最终调试与测试。</li>
<li data-line="18" dir="auto">撰写项目总结报告与论文，准备进行学术汇报与项目展示。</li>
</ul>
</li>
</ol></div></div></div><div class="el-h4 heading-wrapper"><h4 data-heading="3. 与指导老师的交流" dir="auto" class="heading" id="3._与指导老师的交流"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3. 与指导老师的交流</h4><div class="heading-children"><div class="el-p"><p dir="auto">在项目实施过程中，我们与指导老师陈燚教授保持了定期的沟通与反馈。具体交流内容包括：</p></div><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>模型选择与优化</strong>：在选择合适的模型时，我们就多模态特征提取方法、行为识别模型等方面与指导老师进行了深入讨论。老师针对视频特征提取和手部关节检测的技术路线提出了宝贵的意见，帮助我们做出更为合理的决策。</li>
<li data-line="1" dir="auto"><strong>实验设计与数据分析</strong>：每次实验后，我们都会向老师汇报结果，及时获得反馈。通过与老师的讨论，我们能够调整实验策略，更加有效地推进项目。</li>
<li data-line="2" dir="auto"><strong>问题解决与技术支持</strong>：在遇到技术难题时，指导老师为我们提供了很多有价值的建议，尤其是在多模态特征融合和目标检测的实现过程中。老师的指导帮助我们迅速定位问题并提出优化方案。</li>
</ul></div><div class="el-p"><p dir="auto">通过与指导老师的持续交流，我们能够及时纠正项目中的偏差，并根据反馈不断优化项目方案，确保项目按计划顺利推进。</p></div></div></div><div class="el-h4 heading-wrapper"><h4 data-heading="4. 存在的问题" dir="auto" class="heading" id="4._存在的问题"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4. 存在的问题</h4><div class="heading-children"><div class="el-p"><p dir="auto">在项目实施过程中，我们遇到了一些技术难题和挑战，主要包括：</p></div><div class="el-ol"><ol>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>模型的实时性问题</strong>：</p>
<ul>
<li data-line="2" dir="auto">在进行视频特征提取时，尽管VIdeoMae模型能够提供较高的准确性，但其实时性仍然存在一定的瓶颈。在处理长时间的视频流时，模型的响应时间可能会影响到系统的实时反馈能力。为了提高实时性，我们尝试了不同的优化方法，如使用更高效的特征提取模块、降低图像分辨率等，但这些方法仍然存在一定的效果局限。</li>
</ul>
</li>
<li data-line="3" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>手部关键点检测的准确性问题</strong>：</p>
<ul>
<li data-line="5" dir="auto">MediaPipe Hands模型在大多数场景下表现优秀，但在某些特殊情况下（如手部遮挡、低光照环境或者手套颜色与背景颜色相似），模型的准确性可能受到影响。我们尝试通过数据增强和更精细的模型调整来应对这些问题，但仍然面临一定的挑战。</li>
</ul>
</li>
<li data-line="6" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>多模态数据的融合问题</strong>：</p>
<ul>
<li data-line="8" dir="auto">多模态特征融合是提高模型精度的关键，但由于不同模态之间的数据类型和结构差异较大，如何有效融合这些数据一直是一个难题。我们尝试了多种融合方法（如加权融合、拼接融合等），但依然需要进一步优化融合策略，以达到最佳效果。</li>
</ul>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>目标检测的准确性问题</strong>：</p>
<ul>
<li data-line="11" dir="auto">尽管目标检测模型在静态场景下能够取得较好的效果，但在实验过程中，尤其是在设备快速移动或背景复杂的情况下，模型的识别准确性有所下降。我们正计划通过改进训练数据的质量和多样性来提升目标检测的稳定性。</li>
</ul>
</li>
<li data-line="12" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>系统的稳定性问题</strong>：</p>
<ul>
<li data-line="14" dir="auto">在系统集成过程中，多个模块的兼容性和稳定性问题逐渐显现。例如，视频流处理与手部特征提取之间的协调性，以及行为识别与目标检测模块的联动等问题都需要在后期不断调试和优化。</li>
</ul>
</li>
</ol></div><div class="el-p"><p dir="auto">尽管面临这些问题，我们通过调整实验方案和模型优化策略，逐步克服了部分技术难关。未来，我们计划继续优化各个环节的技术，确保系统在实际应用中的稳定性和高效性。</p></div></div></div><div class="el-h4 heading-wrapper"><h4 data-heading="5. 结论" dir="auto" class="heading" id="5._结论"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>5. 结论</h4><div class="heading-children"><div class="el-p"><p dir="auto">总体而言，本项目在多模态特征提取、行为识别、目标检测和实验评价系统的构建等方面取得了一定的进展。虽然在实现过程中遇到了一些技术瓶颈和挑战，但通过与指导老师的积极沟通与合作，我们不断调整和优化了方案，推动了项目的顺利进行。未来，我们将继续优化模型的准确性和实时性，进一步提高系统的稳定性，力争在最终测试中取得更好的结果。</p></div><div class="mod-footer mod-ui"></div></div></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="project\大创\大创中期答辩.html#大创中期答辩"><div class="tree-item-contents heading-link" heading-name="大创中期答辩"><span class="tree-item-title">大创中期答辩</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="project\大创\大创中期答辩.html#大创中期答辩要点"><div class="tree-item-contents heading-link" heading-name="大创中期答辩要点"><span class="tree-item-title">大创中期答辩要点</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="project\大创\大创中期答辩.html#调研结果"><div class="tree-item-contents heading-link" heading-name="调研结果"><span class="tree-item-title">调研结果</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="5"><a class="tree-link" href="project\大创\大创中期答辩.html#1._**Gesture_Recognition_Using_Graph_Convolutional_Networks**"><div class="tree-item-contents heading-link" heading-name="1. **Gesture Recognition Using Graph Convolutional Networks**"><span class="tree-item-title">1. 
Gesture Recognition Using Graph Convolutional Networks
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="5"><a class="tree-link" href="project\大创\大创中期答辩.html#2._**Hand_Gesture_Recognition_Using_Graph_Convolutional_Networks**"><div class="tree-item-contents heading-link" heading-name="2. **Hand Gesture Recognition Using Graph Convolutional Networks**"><span class="tree-item-title">2. 
Hand Gesture Recognition Using Graph Convolutional Networks
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="5"><a class="tree-link" href="project\大创\大创中期答辩.html#3._**Learning_Hand_Pose_Estimation_and_Gesture_Recognition_with_GCN**"><div class="tree-item-contents heading-link" heading-name="3. **Learning Hand Pose Estimation and Gesture Recognition with GCN**"><span class="tree-item-title">3. 
Learning Hand Pose Estimation and Gesture Recognition with GCN
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="5"><a class="tree-link" href="project\大创\大创中期答辩.html#4._**3D_Hand_Gesture_Recognition_Using_Graph_Convolutional_Networks**"><div class="tree-item-contents heading-link" heading-name="4. **3D Hand Gesture Recognition Using Graph Convolutional Networks**"><span class="tree-item-title">4. 
3D Hand Gesture Recognition Using Graph Convolutional Networks
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="5"><a class="tree-link" href="project\大创\大创中期答辩.html#5._**Dynamic_Graph_CNN_for_Gesture_Recognition**"><div class="tree-item-contents heading-link" heading-name="5. **Dynamic Graph CNN for Gesture Recognition**"><span class="tree-item-title">5. 
Dynamic Graph CNN for Gesture Recognition
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="5"><a class="tree-link" href="project\大创\大创中期答辩.html#6._**Graph_Neural_Networks_for_Hand_Gesture_Recognition_in_Video**"><div class="tree-item-contents heading-link" heading-name="6. **Graph Neural Networks for Hand Gesture Recognition in Video**"><span class="tree-item-title">6. 
Graph Neural Networks for Hand Gesture Recognition in Video
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="project\大创\大创中期答辩.html#结论"><div class="tree-item-contents heading-link" heading-name="结论"><span class="tree-item-title">结论</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="project\大创\大创中期答辩.html#项目实施情况总结"><div class="tree-item-contents heading-link" heading-name="项目实施情况总结"><span class="tree-item-title">项目实施情况总结</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="project\大创\大创中期答辩.html#1._研究内容"><div class="tree-item-contents heading-link" heading-name="1. 研究内容"><span class="tree-item-title">1. 
研究内容
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="project\大创\大创中期答辩.html#2._时间安排"><div class="tree-item-contents heading-link" heading-name="2. 时间安排"><span class="tree-item-title">2. 
时间安排
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="project\大创\大创中期答辩.html#3._与指导老师的交流"><div class="tree-item-contents heading-link" heading-name="3. 与指导老师的交流"><span class="tree-item-title">3. 
与指导老师的交流
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="project\大创\大创中期答辩.html#4._存在的问题"><div class="tree-item-contents heading-link" heading-name="4. 存在的问题"><span class="tree-item-title">4. 
存在的问题
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="project\大创\大创中期答辩.html#5._结论"><div class="tree-item-contents heading-link" heading-name="5. 结论"><span class="tree-item-title">5. 
结论
</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>