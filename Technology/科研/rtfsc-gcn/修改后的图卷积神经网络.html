<!DOCTYPE html> <html><head>
		<title>修改后的图卷积神经网络</title>
		<base href="..\..\../">
		<meta id="root-path" root-path="..\..\../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="note - 修改后的图卷积神经网络">
		<meta property="og:title" content="修改后的图卷积神经网络">
		<meta property="og:description" content="note - 修改后的图卷积神经网络">
		<meta property="og:type" content="website">
		<meta property="og:url" content="technology/科研/rtfsc-gcn/修改后的图卷积神经网络.html">
		<meta property="og:image" content="undefined">
		<meta property="og:site_name" content="note">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="stylesheet" href="lib/styles/theme.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager theme-dark show-inline-title show-ribbon"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles">mjx-c.mjx-c1D407.TEX-B::before { padding: 0.686em 0.9em 0px 0px; content: "H"; }
mjx-c.mjx-c1D459.TEX-I::before { padding: 0.694em 0.298em 0.011em 0px; content: "l"; }
mjx-c.mjx-c1D448.TEX-I::before { padding: 0.683em 0.767em 0.022em 0px; content: "U"; }
mjx-mfrac { display: inline-block; text-align: left; }
mjx-frac { display: inline-block; vertical-align: 0.17em; padding: 0px 0.22em; }
mjx-frac[type="d"] { vertical-align: 0.04em; }
mjx-frac[delims] { padding: 0px 0.1em; }
mjx-frac[atop] { padding: 0px 0.12em; }
mjx-frac[atop][delims] { padding: 0px; }
mjx-dtable { display: inline-table; width: 100%; }
mjx-dtable > * { font-size: 2000%; }
mjx-dbox { display: block; font-size: 5%; }
mjx-num { display: block; text-align: center; }
mjx-den { display: block; text-align: center; }
mjx-mfrac[bevelled] > mjx-num { display: inline-block; }
mjx-mfrac[bevelled] > mjx-den { display: inline-block; }
mjx-den[align="right"], mjx-num[align="right"] { text-align: right; }
mjx-den[align="left"], mjx-num[align="left"] { text-align: left; }
mjx-nstrut { display: inline-block; height: 0.054em; width: 0px; vertical-align: -0.054em; }
mjx-nstrut[type="d"] { height: 0.217em; vertical-align: -0.217em; }
mjx-dstrut { display: inline-block; height: 0.505em; width: 0px; }
mjx-dstrut[type="d"] { height: 0.726em; }
mjx-line { display: block; box-sizing: border-box; min-height: 1px; height: 0.06em; border-top: 0.06em solid; margin: 0.06em -0.1em; overflow: hidden; }
mjx-line[type="d"] { margin: 0.18em -0.1em; }
mjx-mtext { display: inline-block; text-align: left; }
mjx-mrow { display: inline-block; text-align: left; }
mjx-mtable { display: inline-block; text-align: center; vertical-align: 0.25em; position: relative; box-sizing: border-box; border-spacing: 0px; border-collapse: collapse; }
mjx-mstyle[size="s"] mjx-mtable { vertical-align: 0.354em; }
mjx-labels { position: absolute; left: 0px; top: 0px; }
mjx-table { display: inline-block; vertical-align: -0.5ex; box-sizing: border-box; }
mjx-table > mjx-itable { vertical-align: middle; text-align: left; box-sizing: border-box; }
mjx-labels > mjx-itable { position: absolute; top: 0px; }
mjx-mtable[justify="left"] { text-align: left; }
mjx-mtable[justify="right"] { text-align: right; }
mjx-mtable[justify="left"][side="left"] { padding-right: 0px !important; }
mjx-mtable[justify="left"][side="right"] { padding-left: 0px !important; }
mjx-mtable[justify="right"][side="left"] { padding-right: 0px !important; }
mjx-mtable[justify="right"][side="right"] { padding-left: 0px !important; }
mjx-mtable[align] { vertical-align: baseline; }
mjx-mtable[align="top"] > mjx-table { vertical-align: top; }
mjx-mtable[align="bottom"] > mjx-table { vertical-align: bottom; }
mjx-mtable[side="right"] mjx-labels { min-width: 100%; }
mjx-mtr { display: table-row; text-align: left; }
mjx-mtr[rowalign="top"] > mjx-mtd { vertical-align: top; }
mjx-mtr[rowalign="center"] > mjx-mtd { vertical-align: middle; }
mjx-mtr[rowalign="bottom"] > mjx-mtd { vertical-align: bottom; }
mjx-mtr[rowalign="baseline"] > mjx-mtd { vertical-align: baseline; }
mjx-mtr[rowalign="axis"] > mjx-mtd { vertical-align: 0.25em; }
mjx-mtd { display: table-cell; text-align: center; padding: 0.215em 0.4em; }
mjx-mtd:first-child { padding-left: 0px; }
mjx-mtd:last-child { padding-right: 0px; }
mjx-mtable > * > mjx-itable > :first-child > mjx-mtd { padding-top: 0px; }
mjx-mtable > * > mjx-itable > :last-child > mjx-mtd { padding-bottom: 0px; }
mjx-tstrut { display: inline-block; height: 1em; vertical-align: -0.25em; }
mjx-labels[align="left"] > mjx-mtr > mjx-mtd { text-align: left; }
mjx-labels[align="right"] > mjx-mtr > mjx-mtd { text-align: right; }
mjx-mtd[extra] { padding: 0px; }
mjx-mtd[rowalign="top"] { vertical-align: top; }
mjx-mtd[rowalign="center"] { vertical-align: middle; }
mjx-mtd[rowalign="bottom"] { vertical-align: bottom; }
mjx-mtd[rowalign="baseline"] { vertical-align: baseline; }
mjx-mtd[rowalign="axis"] { vertical-align: 0.25em; }
mjx-munder { display: inline-block; text-align: left; }
mjx-over { text-align: left; }
mjx-munder:not([limits="false"]) { display: inline-table; }
mjx-munder > mjx-row { text-align: left; }
mjx-under { padding-bottom: 0.1em; }
mjx-stretchy-v.mjx-c5B mjx-beg mjx-c::before { content: "⎡"; padding: 1.154em 0.667em 0.645em 0px; }
mjx-stretchy-v.mjx-c5B mjx-ext mjx-c::before { content: "⎢"; width: 0.667em; }
mjx-stretchy-v.mjx-c5B mjx-end mjx-c::before { content: "⎣"; padding: 1.155em 0.667em 0.644em 0px; }
mjx-stretchy-v.mjx-c5B > mjx-end { margin-top: -1.799em; }
mjx-stretchy-v.mjx-c5B > mjx-ext { border-top-width: 1.769em; border-bottom-width: 1.769em; }
mjx-stretchy-v.mjx-c5D mjx-beg mjx-c::before { content: "⎤"; padding: 1.154em 0.667em 0.645em 0px; }
mjx-stretchy-v.mjx-c5D mjx-ext mjx-c::before { content: "⎥"; width: 0.667em; }
mjx-stretchy-v.mjx-c5D mjx-end mjx-c::before { content: "⎦"; padding: 1.155em 0.667em 0.644em 0px; }
mjx-stretchy-v.mjx-c5D > mjx-end { margin-top: -1.799em; }
mjx-stretchy-v.mjx-c5D > mjx-ext { border-top-width: 1.769em; border-bottom-width: 1.769em; }
mjx-c.mjx-c1D452.TEX-I::before { padding: 0.442em 0.466em 0.011em 0px; content: "e"; }
mjx-c.mjx-c2295::before { padding: 0.583em 0.778em 0.083em 0px; content: "⊕"; }
mjx-c.mjx-c1D417.TEX-B::before { padding: 0.686em 0.869em 0px 0px; content: "X"; }
mjx-c.mjx-c1D70E.TEX-I::before { padding: 0.431em 0.571em 0.011em 0px; content: "σ"; }
mjx-c.mjx-c5E::before { padding: 0.694em 0.5em 0px 0px; content: "^"; }
mjx-c.mjx-c1D400.TEX-B::before { padding: 0.698em 0.869em 0px 0px; content: "A"; }
mjx-c.mjx-c1D416.TEX-B::before { padding: 0.686em 1.189em 0.007em 0px; content: "W"; }
mjx-c.mjx-c1D441.TEX-I::before { padding: 0.683em 0.888em 0px 0px; content: "N"; }
mjx-c.mjx-c1D439.TEX-I::before { padding: 0.68em 0.749em 0px 0px; content: "F"; }
mjx-c.mjx-c1D403.TEX-B::before { padding: 0.686em 0.882em 0px 0px; content: "D"; }
mjx-c.mjx-c7E.TEX-B::before { padding: 0.344em 0.575em 0px 0px; content: "~"; }
mjx-c.mjx-c1D408.TEX-B::before { padding: 0.686em 0.436em 0px 0px; content: "I"; }
mjx-c.mjx-c6E::before { padding: 0.442em 0.556em 0px 0px; content: "n"; }
mjx-c.mjx-c6F::before { padding: 0.448em 0.5em 0.01em 0px; content: "o"; }
mjx-c.mjx-c72::before { padding: 0.442em 0.392em 0px 0px; content: "r"; }
mjx-c.mjx-c6D::before { padding: 0.442em 0.833em 0px 0px; content: "m"; }
mjx-c.mjx-c22C5::before { padding: 0.31em 0.278em 0px 0px; content: "⋅"; }
mjx-c.mjx-c1D431.TEX-B::before { padding: 0.444em 0.607em 0px 0px; content: "x"; }
mjx-c.mjx-c33::before { padding: 0.665em 0.5em 0.022em 0px; content: "3"; }
mjx-c.mjx-c34::before { padding: 0.677em 0.5em 0px 0px; content: "4"; }
mjx-c.mjx-c36::before { padding: 0.666em 0.5em 0.022em 0px; content: "6"; }
mjx-c.mjx-c37::before { padding: 0.676em 0.5em 0.022em 0px; content: "7"; }
mjx-c.mjx-c38::before { padding: 0.666em 0.5em 0.022em 0px; content: "8"; }
mjx-c.mjx-c39::before { padding: 0.666em 0.5em 0.022em 0px; content: "9"; }
mjx-c.mjx-c2033::before { padding: 0.56em 0.55em 0px 0px; content: "′′"; }
mjx-c.mjx-c70::before { padding: 0.442em 0.556em 0.194em 0px; content: "p"; }
mjx-c.mjx-c6C::before { padding: 0.694em 0.278em 0px 0px; content: "l"; }
mjx-c.mjx-c7C::before { padding: 0.75em 0.278em 0.249em 0px; content: "|"; }
mjx-c.mjx-c1D449.TEX-I::before { padding: 0.683em 0.769em 0.022em 0px; content: "V"; }
mjx-c.mjx-c2211.TEX-S2::before { padding: 0.95em 1.444em 0.45em 0px; content: "∑"; }
mjx-c.mjx-c5B.TEX-S3::before { padding: 1.45em 0.528em 0.949em 0px; content: "["; }
mjx-c.mjx-c5D.TEX-S3::before { padding: 1.45em 0.528em 0.949em 0px; content: "]"; }
mjx-c.mjx-c1D432.TEX-B::before { padding: 0.444em 0.607em 0.2em 0px; content: "y"; }
mjx-c.mjx-c73::before { padding: 0.448em 0.394em 0.011em 0px; content: "s"; }
mjx-c.mjx-c66::before { padding: 0.705em 0.372em 0px 0px; content: "f"; }
mjx-c.mjx-c74::before { padding: 0.615em 0.389em 0.01em 0px; content: "t"; }
mjx-c.mjx-c61::before { padding: 0.448em 0.5em 0.011em 0px; content: "a"; }
mjx-c.mjx-c78::before { padding: 0.431em 0.528em 0px 0px; content: "x"; }
mjx-c.mjx-c63::before { padding: 0.448em 0.444em 0.011em 0px; content: "c"; }
mjx-mover { display: inline-block; text-align: left; }
mjx-mover:not([limits="false"]) { padding-top: 0.1em; }
mjx-mover:not([limits="false"]) > * { display: block; text-align: left; }
mjx-c.mjx-c7E::before { padding: 0.318em 0.5em 0px 0px; content: "~"; }
mjx-c.mjx-c1D43A.TEX-I::before { padding: 0.705em 0.786em 0.022em 0px; content: "G"; }
mjx-c.mjx-c1D434.TEX-I::before { padding: 0.716em 0.75em 0px 0px; content: "A"; }
mjx-c.mjx-cD7::before { padding: 0.491em 0.778em 0px 0px; content: "×"; }
mjx-c.mjx-c2260::before { padding: 0.716em 0.778em 0.215em 0px; content: "≠"; }
mjx-c.mjx-c1D446.TEX-I::before { padding: 0.705em 0.645em 0.022em 0px; content: "S"; }
mjx-c.mjx-c7B::before { padding: 0.75em 0.5em 0.25em 0px; content: "{"; }
mjx-c.mjx-c2C::before { padding: 0.121em 0.278em 0.194em 0px; content: ","; }
mjx-c.mjx-c7D::before { padding: 0.75em 0.5em 0.25em 0px; content: "}"; }
mjx-c.mjx-c2208::before { padding: 0.54em 0.667em 0.04em 0px; content: "∈"; }
mjx-c.mjx-c211D.TEX-A::before { padding: 0.683em 0.722em 0px 0px; content: "R"; }
mjx-c.mjx-c1D451.TEX-I::before { padding: 0.694em 0.52em 0.01em 0px; content: "d"; }
mjx-c.mjx-c1D436.TEX-I::before { padding: 0.705em 0.76em 0.022em 0px; content: "C"; }
mjx-c.mjx-c1D450.TEX-I::before { padding: 0.442em 0.433em 0.011em 0px; content: "c"; }
mjx-c.mjx-c35::before { padding: 0.666em 0.5em 0.022em 0px; content: "5"; }
mjx-c.mjx-c1D461.TEX-I::before { padding: 0.626em 0.361em 0.011em 0px; content: "t"; }
mjx-c.mjx-c2032::before { padding: 0.56em 0.275em 0px 0px; content: "′"; }
mjx-c.mjx-c1D706.TEX-I::before { padding: 0.694em 0.583em 0.012em 0px; content: "λ"; }
mjx-c.mjx-c5B::before { padding: 0.75em 0.278em 0.25em 0px; content: "["; }
mjx-c.mjx-c5D::before { padding: 0.75em 0.278em 0.25em 0px; content: "]"; }
mjx-texatom { display: inline-block; text-align: left; }
mjx-msub { display: inline-block; text-align: left; }
mjx-msup { display: inline-block; text-align: left; }
mjx-msubsup { display: inline-block; text-align: left; }
mjx-script { display: inline-block; padding-right: 0.05em; padding-left: 0.033em; }
mjx-script > mjx-spacer { display: block; }
mjx-c.mjx-c2F::before { padding: 0.75em 0.5em 0.25em 0px; content: "/"; }
mjx-c.mjx-c2211.TEX-S1::before { padding: 0.75em 1.056em 0.25em 0px; content: "∑"; }
mjx-c.mjx-c28::before { padding: 0.75em 0.389em 0.25em 0px; content: "("; }
mjx-c.mjx-c1D466.TEX-I::before { padding: 0.442em 0.49em 0.205em 0px; content: "y"; }
mjx-c.mjx-c1D456.TEX-I::before { padding: 0.661em 0.345em 0.011em 0px; content: "i"; }
mjx-c.mjx-c1D703.TEX-I::before { padding: 0.705em 0.469em 0.01em 0px; content: "θ"; }
mjx-c.mjx-c1D447.TEX-I::before { padding: 0.677em 0.704em 0px 0px; content: "T"; }
mjx-c.mjx-c29::before { padding: 0.75em 0.389em 0.25em 0px; content: ")"; }
mjx-c.mjx-c1D6FC.TEX-I::before { padding: 0.442em 0.64em 0.011em 0px; content: "α"; }
mjx-c.mjx-c1D457.TEX-I::before { padding: 0.661em 0.412em 0.204em 0px; content: "j"; }
mjx-c.mjx-c1D44A.TEX-I::before { padding: 0.683em 1.048em 0.022em 0px; content: "W"; }
mjx-c.mjx-c1D44C.TEX-I::before { padding: 0.683em 0.763em 0px 0px; content: "Y"; }
mjx-c.mjx-c1D44B.TEX-I::before { padding: 0.683em 0.852em 0px 0px; content: "X"; }
mjx-c.mjx-c1D437.TEX-I::before { padding: 0.683em 0.828em 0px 0px; content: "D"; }
mjx-c.mjx-c1D45A.TEX-I::before { padding: 0.442em 0.878em 0.011em 0px; content: "m"; }
mjx-c.mjx-c1D43F.TEX-I::before { padding: 0.683em 0.681em 0px 0px; content: "L"; }
mjx-container[jax="CHTML"] { line-height: 0; }
mjx-container [space="1"] { margin-left: 0.111em; }
mjx-container [space="2"] { margin-left: 0.167em; }
mjx-container [space="3"] { margin-left: 0.222em; }
mjx-container [space="4"] { margin-left: 0.278em; }
mjx-container [space="5"] { margin-left: 0.333em; }
mjx-container [rspace="1"] { margin-right: 0.111em; }
mjx-container [rspace="2"] { margin-right: 0.167em; }
mjx-container [rspace="3"] { margin-right: 0.222em; }
mjx-container [rspace="4"] { margin-right: 0.278em; }
mjx-container [rspace="5"] { margin-right: 0.333em; }
mjx-container [size="s"] { font-size: 70.7%; }
mjx-container [size="ss"] { font-size: 50%; }
mjx-container [size="Tn"] { font-size: 60%; }
mjx-container [size="sm"] { font-size: 85%; }
mjx-container [size="lg"] { font-size: 120%; }
mjx-container [size="Lg"] { font-size: 144%; }
mjx-container [size="LG"] { font-size: 173%; }
mjx-container [size="hg"] { font-size: 207%; }
mjx-container [size="HG"] { font-size: 249%; }
mjx-container [width="full"] { width: 100%; }
mjx-box { display: inline-block; }
mjx-block { display: block; }
mjx-itable { display: inline-table; }
mjx-row { display: table-row; }
mjx-row > * { display: table-cell; }
mjx-mtext { display: inline-block; }
mjx-mstyle { display: inline-block; }
mjx-merror { display: inline-block; color: red; background-color: yellow; }
mjx-mphantom { visibility: hidden; }
mjx-assistive-mml { top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); user-select: none; position: absolute !important; padding: 1px 0px 0px !important; border: 0px !important; display: block !important; width: auto !important; overflow: hidden !important; }
mjx-assistive-mml[display="block"] { width: 100% !important; }
mjx-math { display: inline-block; text-align: left; line-height: 0; text-indent: 0px; font-style: normal; font-weight: normal; font-size: 100%; letter-spacing: normal; border-collapse: collapse; overflow-wrap: normal; word-spacing: normal; white-space: nowrap; direction: ltr; padding: 1px 0px; }
mjx-container[jax="CHTML"][display="true"] { display: block; text-align: center; margin: 1em 0px; }
mjx-container[jax="CHTML"][display="true"][width="full"] { display: flex; }
mjx-container[jax="CHTML"][display="true"] mjx-math { padding: 0px; }
mjx-container[jax="CHTML"][justify="left"] { text-align: left; }
mjx-container[jax="CHTML"][justify="right"] { text-align: right; }
mjx-mi { display: inline-block; text-align: left; }
mjx-c { display: inline-block; }
mjx-utext { display: inline-block; padding: 0.75em 0px 0.2em; }
mjx-mn { display: inline-block; text-align: left; }
mjx-mo { display: inline-block; text-align: left; }
mjx-stretchy-h { display: inline-table; width: 100%; }
mjx-stretchy-h > * { display: table-cell; width: 0px; }
mjx-stretchy-h > * > mjx-c { display: inline-block; transform: scaleX(1); }
mjx-stretchy-h > * > mjx-c::before { display: inline-block; width: initial; }
mjx-stretchy-h > mjx-ext { overflow: clip visible; width: 100%; }
mjx-stretchy-h > mjx-ext > mjx-c::before { transform: scaleX(500); }
mjx-stretchy-h > mjx-ext > mjx-c { width: 0px; }
mjx-stretchy-h > mjx-beg > mjx-c { margin-right: -0.1em; }
mjx-stretchy-h > mjx-end > mjx-c { margin-left: -0.1em; }
mjx-stretchy-v { display: inline-block; }
mjx-stretchy-v > * { display: block; }
mjx-stretchy-v > mjx-beg { height: 0px; }
mjx-stretchy-v > mjx-end > mjx-c { display: block; }
mjx-stretchy-v > * > mjx-c { transform: scaleY(1); transform-origin: left center; overflow: hidden; }
mjx-stretchy-v > mjx-ext { display: block; height: 100%; box-sizing: border-box; border: 0px solid transparent; overflow: visible clip; }
mjx-stretchy-v > mjx-ext > mjx-c::before { width: initial; box-sizing: border-box; }
mjx-stretchy-v > mjx-ext > mjx-c { transform: scaleY(500) translateY(0.075em); overflow: visible; }
mjx-mark { display: inline-block; height: 0px; }
mjx-c::before { display: block; width: 0px; }
.MJX-TEX { font-family: MJXZERO, MJXTEX; }
.TEX-B { font-family: MJXZERO, MJXTEX-B; }
.TEX-I { font-family: MJXZERO, MJXTEX-I; }
.TEX-MI { font-family: MJXZERO, MJXTEX-MI; }
.TEX-BI { font-family: MJXZERO, MJXTEX-BI; }
.TEX-S1 { font-family: MJXZERO, MJXTEX-S1; }
.TEX-S2 { font-family: MJXZERO, MJXTEX-S2; }
.TEX-S3 { font-family: MJXZERO, MJXTEX-S3; }
.TEX-S4 { font-family: MJXZERO, MJXTEX-S4; }
.TEX-A { font-family: MJXZERO, MJXTEX-A; }
.TEX-C { font-family: MJXZERO, MJXTEX-C; }
.TEX-CB { font-family: MJXZERO, MJXTEX-CB; }
.TEX-FR { font-family: MJXZERO, MJXTEX-FR; }
.TEX-FRB { font-family: MJXZERO, MJXTEX-FRB; }
.TEX-SS { font-family: MJXZERO, MJXTEX-SS; }
.TEX-SSB { font-family: MJXZERO, MJXTEX-SSB; }
.TEX-SSI { font-family: MJXZERO, MJXTEX-SSI; }
.TEX-SC { font-family: MJXZERO, MJXTEX-SC; }
.TEX-T { font-family: MJXZERO, MJXTEX-T; }
.TEX-V { font-family: MJXZERO, MJXTEX-V; }
.TEX-VB { font-family: MJXZERO, MJXTEX-VB; }
mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c { font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A !important; }
@font-face { font-family: MJXZERO; src: url("lib/fonts/mathjax_zero.woff") format("woff"); }
@font-face { font-family: MJXTEX; src: url("lib/fonts/mathjax_main-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-B; src: url("lib/fonts/mathjax_main-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-I; src: url("lib/fonts/mathjax_math-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-MI; src: url("lib/fonts/mathjax_main-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-BI; src: url("lib/fonts/mathjax_math-bolditalic.woff") format("woff"); }
@font-face { font-family: MJXTEX-S1; src: url("lib/fonts/mathjax_size1-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S2; src: url("lib/fonts/mathjax_size2-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S3; src: url("lib/fonts/mathjax_size3-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S4; src: url("lib/fonts/mathjax_size4-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-A; src: url("lib/fonts/mathjax_ams-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-C; src: url("lib/fonts/mathjax_calligraphic-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-CB; src: url("lib/fonts/mathjax_calligraphic-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-FR; src: url("lib/fonts/mathjax_fraktur-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-FRB; src: url("lib/fonts/mathjax_fraktur-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SS; src: url("lib/fonts/mathjax_sansserif-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSB; src: url("lib/fonts/mathjax_sansserif-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSI; src: url("lib/fonts/mathjax_sansserif-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-SC; src: url("lib/fonts/mathjax_script-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-T; src: url("lib/fonts/mathjax_typewriter-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-V; src: url("lib/fonts/mathjax_vector-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-VB; src: url("lib/fonts/mathjax_vector-bold.woff") format("woff"); }
mjx-c.mjx-c1D464.TEX-I::before { padding: 0.443em 0.716em 0.011em 0px; content: "w"; }
mjx-c.mjx-c31::before { padding: 0.666em 0.5em 0px 0px; content: "1"; }
mjx-c.mjx-c1D465.TEX-I::before { padding: 0.442em 0.572em 0.011em 0px; content: "x"; }
mjx-c.mjx-c2B::before { padding: 0.583em 0.778em 0.082em 0px; content: "+"; }
mjx-c.mjx-c32::before { padding: 0.666em 0.5em 0px 0px; content: "2"; }
mjx-c.mjx-c2E::before { padding: 0.12em 0.278em 0px 0px; content: "."; }
mjx-c.mjx-c1D45B.TEX-I::before { padding: 0.442em 0.6em 0.011em 0px; content: "n"; }
mjx-c.mjx-c2217::before { padding: 0.465em 0.5em 0px 0px; content: "∗"; }
mjx-c.mjx-c2212::before { padding: 0.583em 0.778em 0.082em 0px; content: "−"; }
mjx-c.mjx-c1D44F.TEX-I::before { padding: 0.694em 0.429em 0.011em 0px; content: "b"; }
mjx-c.mjx-c3D::before { padding: 0.583em 0.778em 0.082em 0px; content: "="; }
mjx-c.mjx-c30::before { padding: 0.666em 0.5em 0.022em 0px; content: "0"; }
</style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="修改后的图卷积神经网络">修改后的图卷积神经网络</h1><div class="el-h3 heading-wrapper"><h3 data-heading="修改后的GCN(更加注重图结构)" dir="auto" class="heading" id="修改后的GCN(更加注重图结构)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>修改后的GCN(更加注重图结构)</h3><div class="heading-children"><div class="el-pre"><pre class="language-python" tabindex="0"><code data-line="0" class="language-python is-loaded"><span class="token comment">### MLP with linear output</span>
<span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">'''
            num_layers: 神经网络层数（不包括输入层）。如果 num_layers=1，则简化为线性模型。
            input_dim: 输入特征的维度。
            hidden_dim: 所有隐藏层的单元维度。
            output_dim: 预测的类别数。
        '''</span>

        <span class="token builtin">super</span><span class="token punctuation">(</span>MLP<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 默认为线性模型</span>
        self<span class="token punctuation">.</span>linear_or_not <span class="token operator">=</span> <span class="token boolean">True</span> 
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers

        <span class="token comment"># 检查层数是否为正数</span>
        <span class="token keyword">if</span> num_layers <span class="token operator">&lt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"神经网络的层数应该为正数！"</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> num_layers <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token comment"># 线性模型</span>
            self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 多层模型</span>
            self<span class="token punctuation">.</span>linear_or_not <span class="token operator">=</span> <span class="token boolean">False</span>
            self<span class="token punctuation">.</span>linears <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 用于存储每一层的线性变换</span>
            self<span class="token punctuation">.</span>batch_norms <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 用于存储每一层的批量归一化</span>
            
            <span class="token comment"># 第一层从输入维度到隐藏维度</span>
            self<span class="token punctuation">.</span>linears<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
            
            <span class="token comment"># 中间层</span>
            <span class="token keyword">for</span> layer <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>linears<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
            
            <span class="token comment"># 最后一层从隐藏维度到输出维度</span>
            self<span class="token punctuation">.</span>linears<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
            
            <span class="token comment"># 添加批量归一化层</span>
            <span class="token keyword">for</span> layer <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>batch_norms<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>linear_or_not<span class="token punctuation">:</span>
            <span class="token comment"># 如果是线性模型</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果是多层感知机</span>
            h <span class="token operator">=</span> x
            <span class="token keyword">for</span> layer <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                h <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>batch_norms<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 通过线性层、批量归一化层和激活函数</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span>self<span class="token punctuation">.</span>num_layers <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>h<span class="token punctuation">)</span>  <span class="token comment"># 最终的输出层，没有激活函数</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'complete'</span><span class="token punctuation">)</span>
<span class="token comment"># 归一化有向图</span>
<span class="token keyword">def</span> <span class="token function">normalize_digraph</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">:</span>
    Dl <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    num_node <span class="token operator">=</span> A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    Dn <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>num_node<span class="token punctuation">,</span> num_node<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> Dl<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            Dn<span class="token punctuation">[</span>i<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> Dl<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">**</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    AD <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>A<span class="token punctuation">,</span> Dn<span class="token punctuation">)</span>
    <span class="token keyword">return</span> AD

<span class="token comment"># 将密集张量转换为稀疏格式</span>
<span class="token keyword">def</span> <span class="token function">to_sparse</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x_typename <span class="token operator">=</span> torch<span class="token punctuation">.</span>typename<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'.'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
    sparse_tensortype <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sparse<span class="token punctuation">,</span> x_typename<span class="token punctuation">)</span>

    indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>indices<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>  <span class="token comment"># 如果所有元素都是零</span>
        <span class="token keyword">return</span> sparse_tensortype<span class="token punctuation">(</span><span class="token operator">*</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    indices <span class="token operator">=</span> indices<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span>
    values <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">(</span>indices<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>indices<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> sparse_tensortype<span class="token punctuation">(</span>indices<span class="token punctuation">,</span> values<span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 计算图的度矩阵</span>
<span class="token keyword">def</span> <span class="token function">Comp_degree</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">:</span>
    device <span class="token operator">=</span> A<span class="token punctuation">.</span>device
    out_degree <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    in_degree <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    diag <span class="token operator">=</span> torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span>A<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    degree_matrix <span class="token operator">=</span> diag<span class="token operator">*</span>in_degree <span class="token operator">+</span> diag<span class="token operator">*</span>out_degree <span class="token operator">-</span> torch<span class="token punctuation">.</span>diagflat<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>diagonal<span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> degree_matrix

<span class="token comment"># 图卷积层定义</span>
<span class="token keyword">class</span> <span class="token class-name">GraphConv_Ortega</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>GraphConv_Ortega<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_dim <span class="token operator">=</span> in_dim
        self<span class="token punctuation">.</span>out_dim <span class="token operator">=</span> out_dim
        self<span class="token punctuation">.</span>MLP <span class="token operator">=</span> MLP<span class="token punctuation">(</span>num_layers<span class="token punctuation">,</span> in_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">)</span>

        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>MLP<span class="token punctuation">.</span>linears<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
            init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>MLP<span class="token punctuation">.</span>linears<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> features<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>
        b<span class="token punctuation">,</span> n<span class="token punctuation">,</span> d <span class="token operator">=</span> features<span class="token punctuation">.</span>shape
        <span class="token keyword">assert</span> <span class="token punctuation">(</span>d <span class="token operator">==</span> self<span class="token punctuation">.</span>in_dim<span class="token punctuation">)</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            A_norm <span class="token operator">=</span> A
            deg_mat <span class="token operator">=</span> Comp_degree<span class="token punctuation">(</span>A_norm<span class="token punctuation">)</span>
            frac_degree <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>fractional_matrix_power<span class="token punctuation">(</span>deg_mat<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            Laplacian <span class="token operator">=</span> deg_mat <span class="token operator">-</span> A_norm
            Laplacian_norm <span class="token operator">=</span> frac_degree<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Laplacian<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>frac_degree<span class="token punctuation">)</span><span class="token punctuation">)</span>
            landa<span class="token punctuation">,</span> U <span class="token operator">=</span> torch<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>eig<span class="token punctuation">(</span>Laplacian_norm<span class="token punctuation">)</span>

            <span class="token comment"># 只保留实数部分</span>
            U_real <span class="token operator">=</span> U<span class="token punctuation">.</span>real

            repeated_U_t <span class="token operator">=</span> U_real<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            repeated_U <span class="token operator">=</span> U_real<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            repeated_U_t <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            repeated_U <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                A_norm <span class="token operator">=</span> A<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
                deg_mat <span class="token operator">=</span> Comp_degree<span class="token punctuation">(</span>A_norm<span class="token punctuation">)</span>
                frac_degree <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>fractional_matrix_power<span class="token punctuation">(</span>deg_mat<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
                Laplacian <span class="token operator">=</span> deg_mat <span class="token operator">-</span> A_norm
                Laplacian_norm <span class="token operator">=</span> frac_degree<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Laplacian<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>frac_degree<span class="token punctuation">)</span><span class="token punctuation">)</span>
                landa<span class="token punctuation">,</span> U <span class="token operator">=</span> torch<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>eig<span class="token punctuation">(</span>Laplacian_norm<span class="token punctuation">)</span>

                <span class="token comment"># 只保留实数部分</span>
                U_real <span class="token operator">=</span> U<span class="token punctuation">.</span>real

                repeated_U_t<span class="token punctuation">.</span>append<span class="token punctuation">(</span>U_real<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> U_real<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> U_real<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                repeated_U<span class="token punctuation">.</span>append<span class="token punctuation">(</span>U_real<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> U_real<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> U_real<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            repeated_U_t <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>repeated_U_t<span class="token punctuation">)</span>
            repeated_U <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>repeated_U<span class="token punctuation">)</span>

        agg_feats <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>repeated_U_t<span class="token punctuation">,</span> features<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>MLP<span class="token punctuation">(</span>agg_feats<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>out_dim<span class="token punctuation">)</span>
        out <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>repeated_U<span class="token punctuation">,</span> out<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out

<span class="token comment"># 定义图卷积神经网络模型</span>
<span class="token keyword">class</span> <span class="token class-name">Graph_CNN_ortega</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> final_dropout<span class="token punctuation">,</span>
                 graph_pooling_type<span class="token punctuation">,</span> device<span class="token punctuation">,</span> adj<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Graph_CNN_ortega<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>final_dropout <span class="token operator">=</span> final_dropout
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> device
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers
        self<span class="token punctuation">.</span>graph_pooling_type <span class="token operator">=</span> graph_pooling_type
        self<span class="token punctuation">.</span>input_dim <span class="token operator">=</span> input_dim
        self<span class="token punctuation">.</span>hidden_dim <span class="token operator">=</span> hidden_dim

        self<span class="token punctuation">.</span>Adj <span class="token operator">=</span> adj

        self<span class="token punctuation">.</span>GCNs <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>GCNs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>GraphConv_Ortega<span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>GCNs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>GraphConv_Ortega<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>classifier <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
                            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                            nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>self<span class="token punctuation">.</span>final_dropout<span class="token punctuation">)</span><span class="token punctuation">,</span>
                            nn<span class="token punctuation">.</span>PReLU<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">:</span> Data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        :param data: torch_geometric.data.Data object
        """</span>
        <span class="token comment"># 获取 Data 对象中的节点特征和邻接矩阵</span>
        x <span class="token operator">=</span> data<span class="token punctuation">.</span>x  <span class="token comment"># 节点特征: (num_nodes, num_features)</span>
        edge_index <span class="token operator">=</span> data<span class="token punctuation">.</span>edge_index  <span class="token comment"># 边连接信息: (2, num_edges)</span>

        <span class="token comment"># 获取邻接矩阵</span>
        edge_index <span class="token operator">=</span> edge_index<span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        adj <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

        adj<span class="token punctuation">[</span>edge_index<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> edge_index<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
        adj<span class="token punctuation">[</span>edge_index<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> edge_index<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>  <span class="token comment"># 对称邻接矩阵</span>

        <span class="token comment"># 归一化邻接矩阵</span>
        adj <span class="token operator">=</span> normalize_digraph<span class="token punctuation">(</span>adj<span class="token punctuation">)</span>

        <span class="token comment"># 图卷积操作</span>
        h <span class="token operator">=</span> x
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>GCNs<span class="token punctuation">:</span>
            h <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>layer<span class="token punctuation">(</span>h<span class="token punctuation">,</span> adj<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># 池化操作</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>graph_pooling_type <span class="token operator">==</span> <span class="token string">'mean'</span><span class="token punctuation">:</span>
            pooled <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>h<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>graph_pooling_type <span class="token operator">==</span> <span class="token string">'max'</span><span class="token punctuation">:</span>
            pooled <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>h<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>graph_pooling_type <span class="token operator">==</span> <span class="token string">'sum'</span><span class="token punctuation">:</span>
            pooled <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>h<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># 分类</span>
        score <span class="token operator">=</span> self<span class="token punctuation">.</span>classifier<span class="token punctuation">(</span>pooled<span class="token punctuation">)</span>

        <span class="token keyword">return</span> score

</code><button class="copy-code-button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-copy"><rect x="8" y="8" width="14" height="14" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></pre></div></div></div><div class="el-h3 heading-wrapper"><h3 data-heading="传统图卷积" dir="auto" class="heading" id="传统图卷积"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>传统图卷积</h3><div class="heading-children"><div class="el-pre"><pre class="language-python" tabindex="0"><code data-line="0" class="language-python is-loaded"><span class="token keyword">class</span> <span class="token class-name">Base_GCNModel</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Base_GCNModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers
        self<span class="token punctuation">.</span>hidden_dim <span class="token operator">=</span> hidden_dim
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> dropout

        <span class="token comment"># 定义GCN层</span>
        self<span class="token punctuation">.</span>convs <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 用来存储多个 GCNConv 层</span>
        self<span class="token punctuation">.</span>convs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>GCNConv<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 第一层</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>convs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>GCNConv<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 后续层</span>
        
        <span class="token comment"># 全连接层进行分类</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x<span class="token punctuation">,</span> edge_index <span class="token operator">=</span> data<span class="token punctuation">.</span>x<span class="token punctuation">,</span> data<span class="token punctuation">.</span>edge_index

        <span class="token comment"># 多层 GCN + 激活函数 + Dropout</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>convs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> edge_index<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 每层 GCN</span>
            x <span class="token operator">=</span> F<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">,</span> p<span class="token operator">=</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">,</span> training<span class="token operator">=</span>self<span class="token punctuation">.</span>training<span class="token punctuation">)</span>  <span class="token comment"># Dropout</span>
        
        <span class="token comment"># 池化操作，汇聚所有节点特征</span>
        x <span class="token operator">=</span> global_mean_pool<span class="token punctuation">(</span>x<span class="token punctuation">,</span> data<span class="token punctuation">.</span>batch<span class="token punctuation">)</span>  <span class="token comment"># 使用全局平均池化</span>

        <span class="token comment"># 全连接层进行分类</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 激活函数</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">,</span> p<span class="token operator">=</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">,</span> training<span class="token operator">=</span>self<span class="token punctuation">.</span>training<span class="token punctuation">)</span>  <span class="token comment"># Dropout</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 输出层</span>

        <span class="token keyword">return</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 使用log softmax进行分类</span>


</code><button class="copy-code-button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-copy"><rect x="8" y="8" width="14" height="14" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></pre></div><div class="el-p"><p dir="auto">这个卷积层与传统图卷积（GCN）有几个显著的区别：</p></div></div></div><div class="el-h3 heading-wrapper"><h3 data-heading="1. **使用拉普拉斯矩阵和度矩阵的标准化**：" dir="auto" class="heading" id="1._**使用拉普拉斯矩阵和度矩阵的标准化**："><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>1. <strong>使用拉普拉斯矩阵和度矩阵的标准化</strong>：</h3><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>传统GCN</strong>：通常使用简单的图卷积操作，其中节点特征通过邻接矩阵进行加权传播。标准的GCN卷积操作形式为：  
<div class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D407 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.413em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D70E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.514em; margin-bottom: -0.215em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c7E"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D407 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.413em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D416 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.413em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></div>
其中，<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.514em; margin-bottom: -0.215em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c7E"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math></mjx-container></span> 是归一化的邻接矩阵，<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 是度矩阵，<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 是邻接矩阵，<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D407 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-math></mjx-container></span> 是节点特征，<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D416 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-math></mjx-container></span> 是权重矩阵，<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70E TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 是激活函数（如ReLU）。</li>
<li data-line="6" dir="auto"><strong>这个卷积层</strong>：在传统GCN的基础上进行了扩展和修改，使用了<strong>拉普拉斯矩阵</strong>和<strong>度矩阵的分数阶矩阵幂</strong>（fractional power）。具体来说，它计算了图的<strong>拉普拉斯矩阵</strong>（<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>），并对度矩阵进行<strong>分数阶矩阵幂</strong>操作：
<div class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c6D"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.413em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.413em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math></mjx-container></div>
这样做的目的是为了通过度矩阵的分数幂来进一步调节节点特征的传播方式，使得信息传播的机制更加灵活，可能更好地捕捉到图中节点之间的关系。</li>
</ul></div></div></div><div class="el-h3 heading-wrapper"><h3 data-heading="2. **特征聚合的方式**：" dir="auto" class="heading" id="2._**特征聚合的方式**："><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>2. <strong>特征聚合的方式</strong>：</h3><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>传统GCN</strong>：节点特征通过邻接矩阵直接传播，常见的聚合方式包括加和或平均。</li>
<li data-line="1" dir="auto"><strong>这个卷积层</strong>：在每层图卷积操作中，特征通过拉普拉斯矩阵的特征向量（<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D448 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>）进行加权聚合。它首先计算拉普拉斯矩阵的特征值和特征向量，然后使用这些特征向量将节点特征进行加权聚合。通过特征向量的操作，可以理解为将图的结构信息引入特征聚合的过程中，增强了卷积操作对图结构的感知能力。</li>
</ul></div></div></div><div class="el-h3 heading-wrapper"><h3 data-heading="3. **多层感知器（MLP）结合**：" dir="auto" class="heading" id="3._**多层感知器（MLP）结合**："><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3. <strong>多层感知器（MLP）结合</strong>：</h3><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>传统GCN</strong>：通常使用单一的线性变换和非线性激活函数来变换节点特征。</li>
<li data-line="1" dir="auto"><strong>这个卷积层</strong>：在图卷积之后，使用了一个<strong>MLP</strong>（多层感知器）来进一步处理特征。这个MLP通过多个全连接层来映射节点的聚合特征到更高维度，并最终输出节点的表示。MLP的引入增加了模型的非线性变换能力，可以提取更多复杂的特征。</li>
</ul></div></div></div><div class="el-h3 heading-wrapper"><h3 data-heading="4. **对邻接矩阵的处理**：" dir="auto" class="heading" id="4._**对邻接矩阵的处理**："><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4. <strong>对邻接矩阵的处理</strong>：</h3><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>传统GCN</strong>：通常假设邻接矩阵是固定的，且通过简单的标准化（例如归一化）来处理图结构。</li>
<li data-line="1" dir="auto"><strong>这个卷积层</strong>：除了标准的邻接矩阵处理，还根据拉普拉斯矩阵的特征向量和分数阶度矩阵的计算，对邻接矩阵进行了更加复杂的处理。这样处理后，邻接矩阵不仅仅用于图的结构描述，还可以通过它的特征向量和度矩阵的变换来控制节点特征的传播模式。</li>
</ul></div></div></div><div class="el-h3 heading-wrapper"><h3 data-heading="总结：" dir="auto" class="heading" id="总结："><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>总结：</h3><div class="heading-children"><div class="el-ul"><ul>
<li data-line="0" dir="auto"><strong>传统GCN</strong>侧重于通过邻接矩阵的标准化来完成信息传播。</li>
<li data-line="1" dir="auto"><strong>这个卷积层</strong>通过引入拉普拉斯矩阵的特征向量和度矩阵的分数阶矩阵幂操作，增强了对图结构的感知能力。并且，它结合了MLP进一步处理特征信息，提高了模型的表达能力。</li>
</ul></div><div class="el-p"><p dir="auto">因此，这个卷积层相比传统GCN更加复杂且灵活，通过对图的结构信息进行更加精细的处理，可能能够提升模型在一些图数据上的表现。</p></div><div class="mod-footer mod-ui"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="technology\科研\rtfsc-gcn\修改后的图卷积神经网络.html#修改后的图卷积神经网络"><div class="tree-item-contents heading-link" heading-name="修改后的图卷积神经网络"><span class="tree-item-title">修改后的图卷积神经网络</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="technology\科研\rtfsc-gcn\修改后的图卷积神经网络.html#修改后的GCN(更加注重图结构)"><div class="tree-item-contents heading-link" heading-name="修改后的GCN(更加注重图结构)"><span class="tree-item-title">修改后的GCN(更加注重图结构)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="technology\科研\rtfsc-gcn\修改后的图卷积神经网络.html#传统图卷积"><div class="tree-item-contents heading-link" heading-name="传统图卷积"><span class="tree-item-title">传统图卷积</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="technology\科研\rtfsc-gcn\修改后的图卷积神经网络.html#1._**使用拉普拉斯矩阵和度矩阵的标准化**："><div class="tree-item-contents heading-link" heading-name="1. **使用拉普拉斯矩阵和度矩阵的标准化**："><span class="tree-item-title">1. 
使用拉普拉斯矩阵和度矩阵的标准化：
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="technology\科研\rtfsc-gcn\修改后的图卷积神经网络.html#2._**特征聚合的方式**："><div class="tree-item-contents heading-link" heading-name="2. **特征聚合的方式**："><span class="tree-item-title">2. 
特征聚合的方式：
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="technology\科研\rtfsc-gcn\修改后的图卷积神经网络.html#3._**多层感知器（MLP）结合**："><div class="tree-item-contents heading-link" heading-name="3. **多层感知器（MLP）结合**："><span class="tree-item-title">3. 
多层感知器（MLP）结合：
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="technology\科研\rtfsc-gcn\修改后的图卷积神经网络.html#4._**对邻接矩阵的处理**："><div class="tree-item-contents heading-link" heading-name="4. **对邻接矩阵的处理**："><span class="tree-item-title">4. 
对邻接矩阵的处理：
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="technology\科研\rtfsc-gcn\修改后的图卷积神经网络.html#总结："><div class="tree-item-contents heading-link" heading-name="总结："><span class="tree-item-title">总结：</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>